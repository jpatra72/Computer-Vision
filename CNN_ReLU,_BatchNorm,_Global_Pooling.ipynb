{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra72/Computer-Vision/blob/main/CNN_ReLU%2C_BatchNorm%2C_Global_Pooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B28FoMcSOV0O"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "In this exercise you will be introduced to some practical aspects of deep learning in\n",
        "computer vision, including constructing a deep neural network and training it via gradient\n",
        "descent to tackle image classification.\n",
        "\n",
        "We will use the popular TensorFlow framework through the Keras API.\n",
        "\n",
        "We will tackle **image classification** through deep learning methods, in particular we will look at\n",
        "\n",
        "* Dataset download and normalization\n",
        "* Softmax regression with stochastic gradient descent and Adam\n",
        "* Multilayer perceptrons with tanh and ReLU\n",
        "* A basic convolutional net\n",
        "* BatchNorm, striding, global average pooling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### TensorBoard Plotting\n",
        "\n",
        "TensorBoard is a web-based tool for drawing pretty plots of quantities we care about during training, such as the loss. We need to choose a folder where these values will be stored (\"logdir\").\n",
        "\n",
        "Start the TensorBoard server by executing e.g. `tensorboard --logdir tensorboard_logs` after you've activated your conda environment. If you change the logdir, also adjust it in the cell below.\n",
        "\n",
        "You can view the graphs by visiting http://localhost:6006 in your browser (6006 is the default port).\n",
        "At first there will be nothing to plot, so it will be empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "duzuOCa5OV0Q"
      },
      "outputs": [],
      "source": [
        "log_root = 'tensorboard_logs'\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "%matplotlib inline\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import cv2\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.regularizers as regularizers\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import tensorflow.keras.callbacks as callbacks\n",
        "import tensorflow.keras.initializers as initializers\n",
        "import tensorflow.keras.preprocessing.image as kerasimage\n",
        "\n",
        "# Just an image plotting function\n",
        "def plot_multiple(images, titles=None, colormap='gray',\n",
        "                  max_columns=np.inf, imwidth=4, imheight=4, share_axes=False):\n",
        "    \"\"\"Plot multiple images as subplots on a grid.\"\"\"\n",
        "    if titles is None:\n",
        "        titles = [''] *len(images)\n",
        "    assert len(images) == len(titles)\n",
        "    n_images = len(images)\n",
        "    n_cols = min(max_columns, n_images)\n",
        "    n_rows = int(np.ceil(n_images / n_cols))\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
        "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
        "\n",
        "    axes = axes.flat\n",
        "    # Hide subplots without content\n",
        "    for ax in axes[n_images:]:\n",
        "        ax.axis('off')\n",
        "        \n",
        "    if not isinstance(colormap, (list,tuple)):\n",
        "        colormaps = [colormap]*n_images\n",
        "    else:\n",
        "        colormaps = colormap\n",
        "\n",
        "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
        "        ax.imshow(image, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        \n",
        "    fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJYMbsvzOV0R"
      },
      "source": [
        "## Dataset Preparation\n",
        "We are going to tackle the classic image classification task using the **CIFAR-10 dataset**, containing 60,000 32x32 RGB images of 10 different classes (50,000 for training and 10,000 for testing). \n",
        "\n",
        "![image.png](cifar.png)\n",
        "\n",
        "The dataset is automatically downloaded if you run the next cell.\n",
        "You may read more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html.\n",
        "\n",
        "A common normalization strategy is to map the image RGB values to the range 0-1 and to subtract the mean training pixel value. Perform this normalization below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsW-34CAOV0R",
        "outputId": "534e9b9e-d582-4cc3-bf9b-6e6303b2c960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(im_train, y_train), (im_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Data normalize to 0-1 range and subtract mean of training pixels\n",
        "im_train = im_train / 255\n",
        "im_test = im_test / 255\n",
        "\n",
        "mean_training_pixel = np.mean(im_train, axis=(0,1,2))\n",
        "x_train = im_train - mean_training_pixel\n",
        "x_test = im_test - mean_training_pixel\n",
        "\n",
        "\n",
        "image_shape = x_train[0].shape\n",
        "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBBsGw_-OV0T"
      },
      "source": [
        "## Softmax Regression w/ Adam Optimizer\n",
        "\n",
        "Before considering convolutional neural networks, let us start with a simpler classifier called softmax regression (a.k.a. multinomial logistic regression). Note that even though the name contains \"regression\", this is a classification model.\n",
        "\n",
        "Softmax regression can be understood as a single-layer neural network. We first flatten our input image to a long vector $\\mathbf{x}$, consisting of $32\\cdot 32\\cdot 3= 3072$ values. Then we predict class probabilities $\\hat{\\mathbf{y}}$ through a fully-connected layer with softmax activation:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = W \\mathbf{x} + \\mathbf{b} \\\\\n",
        "\\hat{y}_c = \\operatorname{softmax}(\\mathbf{z})_c = \\frac{\\exp{z_c}}{\\sum_{\\tilde{c}=1}^{10} \\exp{z_{\\tilde{c}}}}\n",
        "$$\n",
        "\n",
        "Here $z_c$ denotes the $c$th component of the vector $\\mathbf{z}$, called the vector of **logits**.\n",
        "The weights $W$ and biases $\\mathbf{b}$ will be learned during training.\n",
        "\n",
        "### Training\n",
        "\n",
        "We train the model by minimizing a **loss function** averaged over the training data. As we are tackling a classification problem, the **cross-entropy** is a suitable loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}; W, \\mathbf{b}) = - \\sum_{c=1}^{10} y_c \\log{\\hat{y}_c}\n",
        "$$\n",
        "\n",
        "Note that in the above notation the ground-truth $\\mathbf{y}$ is a so-called **one-hot vector**, containing a single 1 component, while the remaining components \n",
        "are zeros. The model's predicted $\\hat{\\mathbf{y}}$ is a vector which also sums to one, but whose components all take continuous values in the range $(0, 1)$.\n",
        "\n",
        "We minimize the loss by **stochastic gradient descent** (SGD). That is, we repeatedly sample mini-batches from the training data and update the parameters (weights and biases) towards the direction of the steepest decrease of the loss averaged over the mini-batch. For example, the weight $w_{ij}$ (an element of the matrix $W$) is updated according to:\n",
        "\n",
        "$$\n",
        "w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}_{CE}} {\\partial w_{ij}},\n",
        "$$\n",
        "\n",
        "with $\\eta$ being the learning rate.\n",
        "\n",
        "### Adam Optimizer\n",
        "There has been a lot of research on improving on the simple stochastic gradient descent algorithm we used above. One of the most popular variants is called **Adam** (https://arxiv.org/abs/1412.6980, \"adaptive moment estimation\"). Its learning rate usually requires less precise tuning, and something in the range of $(10^{-4},10^{-3})$ often works well in practice. Intuitively, this is because the algorithm automatically adapts the learning rate for each weight depending on the gradients.\n",
        "\n",
        "You can run it as follows (the optimizer is passed to Keras's `model.fit` function in `train_model`). The difference is not large for such a simple model, but makes a bigger difference for larger networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptmxokh8OV0T",
        "scrolled": true,
        "outputId": "eec6b407-74df-42f7-e2b8-7c48a9155653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 5s 5ms/step - loss: 1.9146 - accuracy: 0.3384 - val_loss: 1.8270 - val_accuracy: 0.3716\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.8036 - accuracy: 0.3861 - val_loss: 1.7870 - val_accuracy: 0.3847\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.7708 - accuracy: 0.3971 - val_loss: 1.7677 - val_accuracy: 0.3981\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.7498 - accuracy: 0.4052 - val_loss: 1.7534 - val_accuracy: 0.4018\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.7345 - accuracy: 0.4091 - val_loss: 1.7448 - val_accuracy: 0.4012\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.7227 - accuracy: 0.4153 - val_loss: 1.7337 - val_accuracy: 0.4075\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.7132 - accuracy: 0.4186 - val_loss: 1.7315 - val_accuracy: 0.4065\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.7054 - accuracy: 0.4195 - val_loss: 1.7252 - val_accuracy: 0.4033\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6988 - accuracy: 0.4214 - val_loss: 1.7223 - val_accuracy: 0.4057\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6930 - accuracy: 0.4253 - val_loss: 1.7196 - val_accuracy: 0.4062\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6880 - accuracy: 0.4265 - val_loss: 1.7219 - val_accuracy: 0.4081\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6840 - accuracy: 0.4294 - val_loss: 1.7150 - val_accuracy: 0.4055\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6796 - accuracy: 0.4307 - val_loss: 1.7172 - val_accuracy: 0.4078\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6765 - accuracy: 0.4313 - val_loss: 1.7106 - val_accuracy: 0.4109\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6732 - accuracy: 0.4343 - val_loss: 1.7097 - val_accuracy: 0.4094\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6700 - accuracy: 0.4342 - val_loss: 1.7103 - val_accuracy: 0.4075\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6681 - accuracy: 0.4335 - val_loss: 1.7086 - val_accuracy: 0.4109\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6650 - accuracy: 0.4358 - val_loss: 1.7096 - val_accuracy: 0.4076\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6625 - accuracy: 0.4378 - val_loss: 1.7106 - val_accuracy: 0.4077\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6605 - accuracy: 0.4373 - val_loss: 1.7130 - val_accuracy: 0.4064\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6588 - accuracy: 0.4372 - val_loss: 1.7118 - val_accuracy: 0.4099\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6571 - accuracy: 0.4377 - val_loss: 1.7115 - val_accuracy: 0.4098\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6551 - accuracy: 0.4392 - val_loss: 1.7096 - val_accuracy: 0.4083\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6532 - accuracy: 0.4416 - val_loss: 1.7100 - val_accuracy: 0.4087\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6513 - accuracy: 0.4419 - val_loss: 1.7106 - val_accuracy: 0.4108\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6509 - accuracy: 0.4423 - val_loss: 1.7085 - val_accuracy: 0.4099\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6492 - accuracy: 0.4417 - val_loss: 1.7081 - val_accuracy: 0.4093\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6479 - accuracy: 0.4419 - val_loss: 1.7095 - val_accuracy: 0.4090\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6459 - accuracy: 0.4431 - val_loss: 1.7086 - val_accuracy: 0.4093\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6447 - accuracy: 0.4428 - val_loss: 1.7106 - val_accuracy: 0.4105\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6441 - accuracy: 0.4424 - val_loss: 1.7091 - val_accuracy: 0.4107\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6427 - accuracy: 0.4445 - val_loss: 1.7097 - val_accuracy: 0.4089\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6417 - accuracy: 0.4444 - val_loss: 1.7098 - val_accuracy: 0.4103\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6412 - accuracy: 0.4450 - val_loss: 1.7074 - val_accuracy: 0.4140\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6398 - accuracy: 0.4449 - val_loss: 1.7116 - val_accuracy: 0.4110\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6389 - accuracy: 0.4451 - val_loss: 1.7096 - val_accuracy: 0.4103\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6383 - accuracy: 0.4456 - val_loss: 1.7117 - val_accuracy: 0.4073\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6371 - accuracy: 0.4468 - val_loss: 1.7098 - val_accuracy: 0.4138\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6364 - accuracy: 0.4473 - val_loss: 1.7118 - val_accuracy: 0.4124\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6351 - accuracy: 0.4476 - val_loss: 1.7138 - val_accuracy: 0.4096\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6341 - accuracy: 0.4479 - val_loss: 1.7138 - val_accuracy: 0.4087\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6333 - accuracy: 0.4483 - val_loss: 1.7109 - val_accuracy: 0.4129\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6325 - accuracy: 0.4474 - val_loss: 1.7137 - val_accuracy: 0.4111\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6324 - accuracy: 0.4475 - val_loss: 1.7131 - val_accuracy: 0.4096\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6312 - accuracy: 0.4489 - val_loss: 1.7101 - val_accuracy: 0.4131\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6308 - accuracy: 0.4483 - val_loss: 1.7107 - val_accuracy: 0.4142\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6294 - accuracy: 0.4500 - val_loss: 1.7118 - val_accuracy: 0.4121\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6293 - accuracy: 0.4494 - val_loss: 1.7157 - val_accuracy: 0.4083\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6285 - accuracy: 0.4508 - val_loss: 1.7107 - val_accuracy: 0.4107\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6273 - accuracy: 0.4511 - val_loss: 1.7127 - val_accuracy: 0.4083\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6272 - accuracy: 0.4494 - val_loss: 1.7141 - val_accuracy: 0.4114\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6271 - accuracy: 0.4501 - val_loss: 1.7112 - val_accuracy: 0.4131\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6260 - accuracy: 0.4503 - val_loss: 1.7182 - val_accuracy: 0.4079\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6249 - accuracy: 0.4504 - val_loss: 1.7138 - val_accuracy: 0.4096\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.6243 - accuracy: 0.4502 - val_loss: 1.7189 - val_accuracy: 0.4079\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6246 - accuracy: 0.4517 - val_loss: 1.7177 - val_accuracy: 0.4074\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6236 - accuracy: 0.4511 - val_loss: 1.7125 - val_accuracy: 0.4090\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6228 - accuracy: 0.4517 - val_loss: 1.7150 - val_accuracy: 0.4100\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6226 - accuracy: 0.4521 - val_loss: 1.7188 - val_accuracy: 0.4083\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6209 - accuracy: 0.4519 - val_loss: 1.7198 - val_accuracy: 0.4072\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6211 - accuracy: 0.4515 - val_loss: 1.7168 - val_accuracy: 0.4119\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6210 - accuracy: 0.4516 - val_loss: 1.7158 - val_accuracy: 0.4111\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6204 - accuracy: 0.4534 - val_loss: 1.7165 - val_accuracy: 0.4105\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6201 - accuracy: 0.4526 - val_loss: 1.7171 - val_accuracy: 0.4121\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6192 - accuracy: 0.4527 - val_loss: 1.7183 - val_accuracy: 0.4062\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6183 - accuracy: 0.4529 - val_loss: 1.7169 - val_accuracy: 0.4109\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6185 - accuracy: 0.4538 - val_loss: 1.7170 - val_accuracy: 0.4093\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6177 - accuracy: 0.4524 - val_loss: 1.7182 - val_accuracy: 0.4070\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6169 - accuracy: 0.4549 - val_loss: 1.7259 - val_accuracy: 0.4101\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6170 - accuracy: 0.4541 - val_loss: 1.7175 - val_accuracy: 0.4098\n"
          ]
        }
      ],
      "source": [
        "softmax_regression = models.Sequential([\n",
        "    layers.Flatten(input_shape=image_shape),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='linear_adam')\n",
        "\n",
        "def train_model(model, batch_size=128, n_epochs=70,  optimizer=optimizers.SGD, learning_rate=1e-2):\n",
        "    opt = optimizer(lr=learning_rate)\n",
        "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
        "    tensorboard_callback = callbacks.TensorBoard(logdir)\n",
        "    model.fit(x=x_train, y=y_train, verbose=1, epochs=n_epochs, \n",
        "              validation_data=(x_test, y_test), batch_size=batch_size,\n",
        "              callbacks=[tensorboard_callback])\n",
        "    \n",
        "train_model(softmax_regression, optimizer=optimizers.Adam, learning_rate=2e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6-K3QHOOV0T"
      },
      "source": [
        "## Interpreting the Learned Weights\n",
        "\n",
        "Multiplication by the weights $W$ can be interpreted as computing responses to correlation templates per image class.\n",
        "\n",
        "That means, we can reshape the weight array $W$ to a obtain \"template images\".\n",
        "\n",
        "Perform this reshaping and visualize the resulting templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "BUWf5xlAOV0T",
        "scrolled": false,
        "outputId": "1c321c82-647e-4b38-d817-773a6fedd718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3072, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "W, b = softmax_regression.layers[1].get_weights()\n",
        "\n",
        "print(W.shape)\n",
        "\n",
        "# `templates` variable here based on W, with dimensions [10 (class count), height, width, 3 (rgb)]\n",
        "templates = W.reshape(32,32,3,10).transpose(3,0,1,2)\n",
        "\n",
        "# Normalize the templates to the 0-1 range for visualization\n",
        "mini = np.min(templates, axis=(1,2,3), keepdims=True)\n",
        "maxi = np.max(templates, axis=(1,2,3), keepdims=True)\n",
        "rescaled_templates = (templates - mini)/ (maxi-mini)\n",
        "plot_multiple(rescaled_templates, labels, max_columns=5, imwidth=2, imheight=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4fXMW8OV0U"
      },
      "source": [
        "Q: Do they look as you would expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh05BRkFOV0U"
      },
      "source": [
        "Some patterns are visible: frogs are green in the center, horses have large red values in the upper center of the image and have a green spot below. Ship images have blue pixels in the bottom corners, deers are surrounded by green."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhXNFWstOV0U"
      },
      "source": [
        "## Multi-Layer Perceptron\n",
        "\n",
        "Softmax regression has a big limitation: the decision surface between any two classes (i.e. the part of the input space where the classification decision changes from one class to another) is a simple hyperplane (\"flat\").\n",
        "\n",
        "The **multi-layer perceptron** (MLP) is a neural network model with additional layer(s) between the input and the logits (so-called hidden layers), with nonlinear activation functions. Why are activation functions needed?\n",
        "\n",
        "Before the current generation of neural networks, the **hyperbolic tangent** (tanh) function used to be the preferred activation function in the hidden layers of MLPs. It is sigmoid shaped and has a range of $(-1,1)$. Create such a network in Keras and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8VBufOKOV0U",
        "outputId": "89c72114-8dab-4ad7-8d1f-3e326d391609",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 4s 8ms/step - loss: 1.8117 - accuracy: 0.3756 - val_loss: 1.7312 - val_accuracy: 0.4137\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.6812 - accuracy: 0.4239 - val_loss: 1.6638 - val_accuracy: 0.4275\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.6120 - accuracy: 0.4469 - val_loss: 1.6387 - val_accuracy: 0.4359\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.5613 - accuracy: 0.4665 - val_loss: 1.5979 - val_accuracy: 0.4516\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.5131 - accuracy: 0.4844 - val_loss: 1.5749 - val_accuracy: 0.4576\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 1.4685 - accuracy: 0.5015 - val_loss: 1.5568 - val_accuracy: 0.4611\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.4285 - accuracy: 0.5171 - val_loss: 1.5486 - val_accuracy: 0.4642\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3884 - accuracy: 0.5319 - val_loss: 1.5222 - val_accuracy: 0.4817\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3507 - accuracy: 0.5466 - val_loss: 1.5133 - val_accuracy: 0.4814\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3138 - accuracy: 0.5597 - val_loss: 1.5071 - val_accuracy: 0.4797\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2811 - accuracy: 0.5716 - val_loss: 1.4962 - val_accuracy: 0.4862\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2445 - accuracy: 0.5874 - val_loss: 1.4854 - val_accuracy: 0.4868\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2093 - accuracy: 0.5993 - val_loss: 1.4794 - val_accuracy: 0.4896\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1762 - accuracy: 0.6127 - val_loss: 1.4969 - val_accuracy: 0.4861\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1456 - accuracy: 0.6241 - val_loss: 1.4738 - val_accuracy: 0.4988\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1106 - accuracy: 0.6373 - val_loss: 1.4706 - val_accuracy: 0.4952\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0790 - accuracy: 0.6503 - val_loss: 1.4664 - val_accuracy: 0.4962\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0470 - accuracy: 0.6620 - val_loss: 1.4608 - val_accuracy: 0.5021\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0160 - accuracy: 0.6760 - val_loss: 1.4751 - val_accuracy: 0.4910\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9858 - accuracy: 0.6870 - val_loss: 1.4613 - val_accuracy: 0.5034\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9553 - accuracy: 0.7000 - val_loss: 1.4675 - val_accuracy: 0.4986\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9247 - accuracy: 0.7117 - val_loss: 1.4663 - val_accuracy: 0.5021\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8967 - accuracy: 0.7208 - val_loss: 1.4848 - val_accuracy: 0.4946\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8658 - accuracy: 0.7332 - val_loss: 1.4836 - val_accuracy: 0.4937\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8401 - accuracy: 0.7430 - val_loss: 1.4849 - val_accuracy: 0.4958\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.8119 - accuracy: 0.7537 - val_loss: 1.4807 - val_accuracy: 0.4967\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.7846 - accuracy: 0.7640 - val_loss: 1.4919 - val_accuracy: 0.4980\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.7579 - accuracy: 0.7767 - val_loss: 1.4980 - val_accuracy: 0.4970\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.7329 - accuracy: 0.7854 - val_loss: 1.5036 - val_accuracy: 0.4963\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.7084 - accuracy: 0.7944 - val_loss: 1.5158 - val_accuracy: 0.5018\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.6840 - accuracy: 0.8042 - val_loss: 1.5139 - val_accuracy: 0.4993\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.6580 - accuracy: 0.8145 - val_loss: 1.5250 - val_accuracy: 0.4943\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.6364 - accuracy: 0.8219 - val_loss: 1.5342 - val_accuracy: 0.4958\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.6144 - accuracy: 0.8301 - val_loss: 1.5400 - val_accuracy: 0.4913\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5904 - accuracy: 0.8412 - val_loss: 1.5727 - val_accuracy: 0.4868\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5696 - accuracy: 0.8455 - val_loss: 1.5610 - val_accuracy: 0.4957\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5472 - accuracy: 0.8557 - val_loss: 1.5710 - val_accuracy: 0.4923\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5268 - accuracy: 0.8630 - val_loss: 1.5761 - val_accuracy: 0.4962\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5077 - accuracy: 0.8683 - val_loss: 1.5919 - val_accuracy: 0.4919\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.4852 - accuracy: 0.8800 - val_loss: 1.6093 - val_accuracy: 0.4879\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4670 - accuracy: 0.8850 - val_loss: 1.6159 - val_accuracy: 0.4883\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.4488 - accuracy: 0.8922 - val_loss: 1.6294 - val_accuracy: 0.4921\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.4323 - accuracy: 0.8973 - val_loss: 1.6357 - val_accuracy: 0.4911\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.4137 - accuracy: 0.9034 - val_loss: 1.6501 - val_accuracy: 0.4888\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3972 - accuracy: 0.9092 - val_loss: 1.6545 - val_accuracy: 0.4900\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3813 - accuracy: 0.9148 - val_loss: 1.6824 - val_accuracy: 0.4872\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3647 - accuracy: 0.9201 - val_loss: 1.6994 - val_accuracy: 0.4880\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.3497 - accuracy: 0.9251 - val_loss: 1.7030 - val_accuracy: 0.4895\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3337 - accuracy: 0.9300 - val_loss: 1.7330 - val_accuracy: 0.4884\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3201 - accuracy: 0.9354 - val_loss: 1.7436 - val_accuracy: 0.4844\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.3063 - accuracy: 0.9387 - val_loss: 1.7584 - val_accuracy: 0.4806\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2909 - accuracy: 0.9444 - val_loss: 1.7666 - val_accuracy: 0.4835\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2771 - accuracy: 0.9490 - val_loss: 1.7841 - val_accuracy: 0.4827\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2660 - accuracy: 0.9509 - val_loss: 1.8128 - val_accuracy: 0.4870\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2542 - accuracy: 0.9548 - val_loss: 1.8073 - val_accuracy: 0.4859\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2432 - accuracy: 0.9580 - val_loss: 1.8326 - val_accuracy: 0.4833\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2308 - accuracy: 0.9623 - val_loss: 1.8463 - val_accuracy: 0.4828\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2219 - accuracy: 0.9638 - val_loss: 1.8561 - val_accuracy: 0.4863\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2097 - accuracy: 0.9673 - val_loss: 1.8816 - val_accuracy: 0.4826\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1985 - accuracy: 0.9707 - val_loss: 1.9039 - val_accuracy: 0.4794\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1890 - accuracy: 0.9728 - val_loss: 1.9165 - val_accuracy: 0.4837\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1820 - accuracy: 0.9737 - val_loss: 1.9412 - val_accuracy: 0.4875\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1719 - accuracy: 0.9769 - val_loss: 1.9572 - val_accuracy: 0.4819\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.1646 - accuracy: 0.9785 - val_loss: 1.9760 - val_accuracy: 0.4790\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1543 - accuracy: 0.9815 - val_loss: 1.9836 - val_accuracy: 0.4811\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.1470 - accuracy: 0.9824 - val_loss: 2.0099 - val_accuracy: 0.4823\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.1404 - accuracy: 0.9834 - val_loss: 2.0187 - val_accuracy: 0.4817\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1332 - accuracy: 0.9853 - val_loss: 2.0441 - val_accuracy: 0.4802\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.1277 - accuracy: 0.9855 - val_loss: 2.0571 - val_accuracy: 0.4837\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.1196 - accuracy: 0.9873 - val_loss: 2.0782 - val_accuracy: 0.4782\n"
          ]
        }
      ],
      "source": [
        "tanh_mlp = models.Sequential([\n",
        "    layers.Flatten(input_shape=image_shape),\n",
        "    layers.Dense(512, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='tanh_mlp')\n",
        "\n",
        "train_model(tanh_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S876Z5srOV0U"
      },
      "source": [
        "Q: Does it obtain better results than the linear model? What do you observe if you compare the curves for training and validation?\n",
        "\n",
        "Q: How and why does the behaviour of the validation loss differ from the validation accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3w7O8WZOV0U"
      },
      "source": [
        "The validation accuracy gets a bit better compared to the previous model. The training accuracy goes close to 1, but the validation accuracy starts to decrease after about 30 epochs, thus the model overfits to the training data.\n",
        "\n",
        "After 20 epochs, the validation loss gets worse and after 45 epochs the validation loss is even lower compared to the initialized model and it continues the grow. In contrast, the validation accuracy decreases only by a few percent. The latter only counts the number of correctly classified classes, so if the model predicts the correct class with confidence 0.11 and all others with slightly less, the accuracy can still be high. The cross-entropy loss however punishes low confidences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXImOmqOV0U"
      },
      "source": [
        "## ReLU\n",
        "\n",
        "The ReLU activation function has become more popular in recent years, especially for deeper nets. Create and train an MLP that uses ReLU as the activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "500ZdNiQOV0U",
        "outputId": "7c77fce5-f617-4818-ac57-ee08bdd5c389",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.6638 - accuracy: 0.4215 - val_loss: 1.5253 - val_accuracy: 0.4670\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.4627 - accuracy: 0.4940 - val_loss: 1.4517 - val_accuracy: 0.4914\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.3681 - accuracy: 0.5298 - val_loss: 1.4197 - val_accuracy: 0.5054\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2987 - accuracy: 0.5575 - val_loss: 1.3807 - val_accuracy: 0.5163\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.2421 - accuracy: 0.5768 - val_loss: 1.3654 - val_accuracy: 0.5173\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1904 - accuracy: 0.5962 - val_loss: 1.3554 - val_accuracy: 0.5260\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1475 - accuracy: 0.6133 - val_loss: 1.3431 - val_accuracy: 0.5256\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1047 - accuracy: 0.6278 - val_loss: 1.3324 - val_accuracy: 0.5317\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.0658 - accuracy: 0.6438 - val_loss: 1.3167 - val_accuracy: 0.5378\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.0266 - accuracy: 0.6580 - val_loss: 1.3252 - val_accuracy: 0.5362\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9941 - accuracy: 0.6708 - val_loss: 1.3260 - val_accuracy: 0.5394\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9606 - accuracy: 0.6816 - val_loss: 1.3227 - val_accuracy: 0.5391\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9275 - accuracy: 0.6957 - val_loss: 1.3238 - val_accuracy: 0.5428\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8984 - accuracy: 0.7039 - val_loss: 1.3343 - val_accuracy: 0.5413\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8681 - accuracy: 0.7162 - val_loss: 1.3345 - val_accuracy: 0.5377\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8384 - accuracy: 0.7280 - val_loss: 1.3518 - val_accuracy: 0.5351\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.8126 - accuracy: 0.7393 - val_loss: 1.3373 - val_accuracy: 0.5461\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.7847 - accuracy: 0.7473 - val_loss: 1.3327 - val_accuracy: 0.5450\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.7610 - accuracy: 0.7548 - val_loss: 1.3498 - val_accuracy: 0.5436\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.7331 - accuracy: 0.7651 - val_loss: 1.3480 - val_accuracy: 0.5476\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7106 - accuracy: 0.7746 - val_loss: 1.3639 - val_accuracy: 0.5447\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.6850 - accuracy: 0.7855 - val_loss: 1.3655 - val_accuracy: 0.5395\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.6629 - accuracy: 0.7939 - val_loss: 1.3645 - val_accuracy: 0.5459\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.6411 - accuracy: 0.8024 - val_loss: 1.3671 - val_accuracy: 0.5476\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.6201 - accuracy: 0.8113 - val_loss: 1.3906 - val_accuracy: 0.5404\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5968 - accuracy: 0.8191 - val_loss: 1.3857 - val_accuracy: 0.5435\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5777 - accuracy: 0.8261 - val_loss: 1.4103 - val_accuracy: 0.5438\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5581 - accuracy: 0.8327 - val_loss: 1.4048 - val_accuracy: 0.5446\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5392 - accuracy: 0.8398 - val_loss: 1.4244 - val_accuracy: 0.5434\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5229 - accuracy: 0.8453 - val_loss: 1.4201 - val_accuracy: 0.5401\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.5004 - accuracy: 0.8562 - val_loss: 1.4507 - val_accuracy: 0.5417\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4817 - accuracy: 0.8631 - val_loss: 1.4484 - val_accuracy: 0.5426\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4651 - accuracy: 0.8704 - val_loss: 1.4565 - val_accuracy: 0.5449\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4525 - accuracy: 0.8735 - val_loss: 1.4812 - val_accuracy: 0.5449\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4333 - accuracy: 0.8804 - val_loss: 1.5031 - val_accuracy: 0.5355\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4189 - accuracy: 0.8863 - val_loss: 1.4948 - val_accuracy: 0.5396\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.4022 - accuracy: 0.8927 - val_loss: 1.5211 - val_accuracy: 0.5421\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3897 - accuracy: 0.8978 - val_loss: 1.5391 - val_accuracy: 0.5352\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3768 - accuracy: 0.9007 - val_loss: 1.5264 - val_accuracy: 0.5429\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3609 - accuracy: 0.9061 - val_loss: 1.5587 - val_accuracy: 0.5397\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3461 - accuracy: 0.9134 - val_loss: 1.5891 - val_accuracy: 0.5347\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3363 - accuracy: 0.9165 - val_loss: 1.5894 - val_accuracy: 0.5454\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3210 - accuracy: 0.9215 - val_loss: 1.6126 - val_accuracy: 0.5353\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.3103 - accuracy: 0.9247 - val_loss: 1.5973 - val_accuracy: 0.5396\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2972 - accuracy: 0.9297 - val_loss: 1.6756 - val_accuracy: 0.5352\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2840 - accuracy: 0.9333 - val_loss: 1.6453 - val_accuracy: 0.5380\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2759 - accuracy: 0.9362 - val_loss: 1.6712 - val_accuracy: 0.5385\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2657 - accuracy: 0.9388 - val_loss: 1.6892 - val_accuracy: 0.5380\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2517 - accuracy: 0.9447 - val_loss: 1.6903 - val_accuracy: 0.5332\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2435 - accuracy: 0.9474 - val_loss: 1.7018 - val_accuracy: 0.5399\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2315 - accuracy: 0.9508 - val_loss: 1.7295 - val_accuracy: 0.5326\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2258 - accuracy: 0.9516 - val_loss: 1.7448 - val_accuracy: 0.5330\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2144 - accuracy: 0.9567 - val_loss: 1.7573 - val_accuracy: 0.5378\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.2082 - accuracy: 0.9575 - val_loss: 1.7871 - val_accuracy: 0.5349\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1992 - accuracy: 0.9606 - val_loss: 1.7865 - val_accuracy: 0.5350\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1881 - accuracy: 0.9638 - val_loss: 1.8223 - val_accuracy: 0.5363\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1859 - accuracy: 0.9638 - val_loss: 1.8243 - val_accuracy: 0.5328\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1718 - accuracy: 0.9689 - val_loss: 1.8454 - val_accuracy: 0.5314\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1677 - accuracy: 0.9697 - val_loss: 1.8563 - val_accuracy: 0.5326\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1598 - accuracy: 0.9723 - val_loss: 1.8785 - val_accuracy: 0.5333\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1563 - accuracy: 0.9725 - val_loss: 1.9145 - val_accuracy: 0.5250\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1462 - accuracy: 0.9761 - val_loss: 1.9206 - val_accuracy: 0.5348\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1419 - accuracy: 0.9765 - val_loss: 1.9698 - val_accuracy: 0.5240\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 0.1344 - accuracy: 0.9788 - val_loss: 1.9651 - val_accuracy: 0.5281\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1284 - accuracy: 0.9804 - val_loss: 1.9749 - val_accuracy: 0.5332\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1236 - accuracy: 0.9809 - val_loss: 1.9714 - val_accuracy: 0.5335\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1196 - accuracy: 0.9826 - val_loss: 2.0405 - val_accuracy: 0.5299\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1128 - accuracy: 0.9837 - val_loss: 2.0534 - val_accuracy: 0.5252\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1125 - accuracy: 0.9842 - val_loss: 2.0515 - val_accuracy: 0.5292\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1107 - accuracy: 0.9827 - val_loss: 2.0700 - val_accuracy: 0.5304\n"
          ]
        }
      ],
      "source": [
        "relu_mlp = models.Sequential([\n",
        "    layers.Flatten(input_shape=image_shape),\n",
        "    layers.Dense(512, activation='relu', kernel_initializer='he_uniform'),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='relu_mlp')\n",
        "train_model(relu_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWp5hzKOV0U"
      },
      "source": [
        "Do the results change? What benefits does ReLU have against tanh?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf1MyEvxOV0V"
      },
      "source": [
        "The results again get better. Due to the linear behaviour of ReLU for positive values, it has less issues with vanishing gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7KyVRcZOV0V"
      },
      "source": [
        "## A Simple Convolutional Neural Network\n",
        "\n",
        "The previous models did not explicitly make use of the grid structure of the image pixels. Convolutional neural networks do.\n",
        "\n",
        "Instead of reshaping the input image pixels into one long vector, convolutional layers slide small filters across the input, just as with the convolutional filters we saw earlier in the course. In the earlier parts, we looked at convolution on an image with a single channel in case of grayscale images, or channelwise separate convolutions on RGB images.\n",
        "\n",
        "In CNNs, the multiple input channels of a conv layer are not handled independently, but are linearly combined. This means that the weight array has shape `[kernel_height, kernel_width, num_input_channels, num_output_channels]` and we perform a weighted sum along the input channel axis. Another difference is the use of a **bias** vector of shape `[num_output_channels]`, each component of which gets added on the corresponding output channel.\n",
        "\n",
        "As you already know, convolution is a linear operator, so it is possible to express any convolutional layer as a fully-connected layer.\n",
        "However, the convolutional layer's weight matrix is sparse (has many zeros) compared to a fully-connected (\"dense\") layer because each output only depends on a small number of inputs, namely, those within a small neigborhood. Further, the weight values are shared between the different pixel locations.\n",
        "\n",
        "This tutorial has some great visualisations and explanations on the details of conv layers: https://arxiv.org/abs/1603.07285.\n",
        "\n",
        "Assuming a fixed input image size, do you think the reverse of the above also holds? Can any fully-connected layer be expressed as a convolutional layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDSO2tjIOV0V"
      },
      "source": [
        "Yes, the trick is that the convolutional filter needs to have the size of the entire input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX-5J2EEOV0V"
      },
      "source": [
        "Technically, what's called a \"convolutional\" layer is usually implemented as a *cross-correlation* computation. Could there be any advantage in using the actual definition of convolution in these layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxwQFFiIOV0V"
      },
      "source": [
        "No. Since the weights of a filter kernel are all sampled from the same random distribution in the beginning of training, flipping the kernel would make no systematic difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc67XnkNOV0V"
      },
      "source": [
        "Train a simple CNN model with 2 conv layers and final fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YDzuB0NOV0V",
        "outputId": "a8297191-4f1e-491d-de44-0ccae7509375",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 14s 16ms/step - loss: 1.3899 - accuracy: 0.5116 - val_loss: 1.1559 - val_accuracy: 0.5937\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 1.0376 - accuracy: 0.6391 - val_loss: 1.0046 - val_accuracy: 0.6564\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.9037 - accuracy: 0.6883 - val_loss: 0.9075 - val_accuracy: 0.6873\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.8158 - accuracy: 0.7198 - val_loss: 0.9149 - val_accuracy: 0.6857\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.7516 - accuracy: 0.7436 - val_loss: 0.8355 - val_accuracy: 0.7143\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.6998 - accuracy: 0.7609 - val_loss: 0.8526 - val_accuracy: 0.7041\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.6497 - accuracy: 0.7766 - val_loss: 0.8804 - val_accuracy: 0.7042\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.6103 - accuracy: 0.7901 - val_loss: 0.8326 - val_accuracy: 0.7172\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.5735 - accuracy: 0.8025 - val_loss: 0.8163 - val_accuracy: 0.7253\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.5424 - accuracy: 0.8130 - val_loss: 0.8240 - val_accuracy: 0.7250\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.5113 - accuracy: 0.8247 - val_loss: 0.8689 - val_accuracy: 0.7189\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4835 - accuracy: 0.8344 - val_loss: 0.8736 - val_accuracy: 0.7222\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4636 - accuracy: 0.8407 - val_loss: 0.8643 - val_accuracy: 0.7228\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4360 - accuracy: 0.8505 - val_loss: 0.8841 - val_accuracy: 0.7249\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4145 - accuracy: 0.8562 - val_loss: 0.9048 - val_accuracy: 0.7231\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3894 - accuracy: 0.8669 - val_loss: 0.9476 - val_accuracy: 0.7209\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3728 - accuracy: 0.8710 - val_loss: 0.9580 - val_accuracy: 0.7170\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3529 - accuracy: 0.8783 - val_loss: 0.9684 - val_accuracy: 0.7132\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3325 - accuracy: 0.8855 - val_loss: 1.0150 - val_accuracy: 0.7137\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3205 - accuracy: 0.8896 - val_loss: 1.0326 - val_accuracy: 0.7152\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2993 - accuracy: 0.8974 - val_loss: 1.0920 - val_accuracy: 0.7082\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2867 - accuracy: 0.9017 - val_loss: 1.0930 - val_accuracy: 0.7134\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.2673 - accuracy: 0.9089 - val_loss: 1.1247 - val_accuracy: 0.7066\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.2546 - accuracy: 0.9135 - val_loss: 1.1248 - val_accuracy: 0.7113\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.2412 - accuracy: 0.9178 - val_loss: 1.1545 - val_accuracy: 0.7106\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2301 - accuracy: 0.9223 - val_loss: 1.2030 - val_accuracy: 0.7107\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2167 - accuracy: 0.9268 - val_loss: 1.2308 - val_accuracy: 0.7059\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2073 - accuracy: 0.9301 - val_loss: 1.2480 - val_accuracy: 0.7088\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1963 - accuracy: 0.9340 - val_loss: 1.2961 - val_accuracy: 0.7057\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1865 - accuracy: 0.9379 - val_loss: 1.3205 - val_accuracy: 0.7070\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1774 - accuracy: 0.9415 - val_loss: 1.3569 - val_accuracy: 0.7062\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1675 - accuracy: 0.9435 - val_loss: 1.4135 - val_accuracy: 0.7037\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1588 - accuracy: 0.9470 - val_loss: 1.4463 - val_accuracy: 0.7005\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1527 - accuracy: 0.9490 - val_loss: 1.4549 - val_accuracy: 0.7071\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1428 - accuracy: 0.9535 - val_loss: 1.5598 - val_accuracy: 0.6935\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1364 - accuracy: 0.9551 - val_loss: 1.5803 - val_accuracy: 0.6971\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1284 - accuracy: 0.9580 - val_loss: 1.6015 - val_accuracy: 0.6976\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1201 - accuracy: 0.9611 - val_loss: 1.6716 - val_accuracy: 0.6952\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1213 - accuracy: 0.9588 - val_loss: 1.6609 - val_accuracy: 0.6982\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1081 - accuracy: 0.9664 - val_loss: 1.7020 - val_accuracy: 0.6977\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1038 - accuracy: 0.9661 - val_loss: 1.7934 - val_accuracy: 0.6929\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1005 - accuracy: 0.9676 - val_loss: 1.7827 - val_accuracy: 0.6979\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0909 - accuracy: 0.9725 - val_loss: 1.8186 - val_accuracy: 0.6990\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0898 - accuracy: 0.9714 - val_loss: 1.8925 - val_accuracy: 0.6934\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0838 - accuracy: 0.9735 - val_loss: 1.8975 - val_accuracy: 0.6953\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0807 - accuracy: 0.9743 - val_loss: 1.9670 - val_accuracy: 0.6909\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0921 - accuracy: 0.9683 - val_loss: 2.0080 - val_accuracy: 0.6913\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0721 - accuracy: 0.9777 - val_loss: 2.0298 - val_accuracy: 0.6913\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0702 - accuracy: 0.9783 - val_loss: 2.0869 - val_accuracy: 0.6911\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0703 - accuracy: 0.9770 - val_loss: 2.0921 - val_accuracy: 0.6918\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0773 - accuracy: 0.9745 - val_loss: 2.0939 - val_accuracy: 0.6944\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0569 - accuracy: 0.9832 - val_loss: 2.1970 - val_accuracy: 0.6853\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0611 - accuracy: 0.9809 - val_loss: 2.2107 - val_accuracy: 0.6929\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0774 - accuracy: 0.9733 - val_loss: 2.2475 - val_accuracy: 0.6918\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0579 - accuracy: 0.9815 - val_loss: 2.2398 - val_accuracy: 0.6959\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0465 - accuracy: 0.9865 - val_loss: 2.3346 - val_accuracy: 0.6917\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 2.3702 - val_accuracy: 0.6896\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 2.3765 - val_accuracy: 0.6895\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0612 - accuracy: 0.9791 - val_loss: 2.4106 - val_accuracy: 0.6944\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0526 - accuracy: 0.9830 - val_loss: 2.4281 - val_accuracy: 0.6879\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0449 - accuracy: 0.9858 - val_loss: 2.5107 - val_accuracy: 0.6885\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0485 - accuracy: 0.9846 - val_loss: 2.5263 - val_accuracy: 0.6912\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 2.5539 - val_accuracy: 0.6886\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0581 - accuracy: 0.9799 - val_loss: 2.6308 - val_accuracy: 0.6814\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0479 - accuracy: 0.9847 - val_loss: 2.6105 - val_accuracy: 0.6854\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0411 - accuracy: 0.9869 - val_loss: 2.6174 - val_accuracy: 0.6901\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 2.7213 - val_accuracy: 0.6889\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0403 - accuracy: 0.9867 - val_loss: 2.7452 - val_accuracy: 0.6853\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0506 - accuracy: 0.9826 - val_loss: 2.7537 - val_accuracy: 0.6920\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0450 - accuracy: 0.9848 - val_loss: 2.7771 - val_accuracy: 0.6861\n"
          ]
        }
      ],
      "source": [
        "cnn = models.Sequential([\n",
        "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
        "                  kernel_initializer='he_uniform', padding='same', \n",
        "                  input_shape=image_shape),\n",
        "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
        "                  kernel_initializer='he_uniform', padding='same'),\n",
        "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='cnn')\n",
        "\n",
        "train_model(cnn, optimizer=optimizers.Adam, learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26H-58P1OV0V"
      },
      "source": [
        "Q: Does it improve the result? Does it run faster than the MLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnSPUfLrOV0V"
      },
      "source": [
        "The accuracy improves significantly, but the training time increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeEodtQEOV0V"
      },
      "source": [
        "Q: How many parameters does this model have? How many parameters has the MLP? Show the steps of your computation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EVpt2QYOV0V"
      },
      "source": [
        "The number of parameters in the weight of a 2D convolutional filter is $\\text{input_channels} \\cdot \\text{output_channels} \\cdot \\text{kernel_size}^2$. The bias has a parameter for each output channel. Thus the first convolution has $1728+64$ parameters, the second has $36864+64$ parameters. After applying pooling twice, the feature maps have size $8\\times8$ with $64$ channels, after flattening this corresponds to $4096$ features. Thus, the final dense layer has $4096\\cdot10+10=40970$ parameters. Summing everything together gives $79690$ parameters.\n",
        "\n",
        "The MLP receives $32\\cdot32\\cdot3=3072$ features as input, thus the total number of parameters is $3072\\cdot512+512+512\\cdot10+10=1578506$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39QBi8ZwOV0W"
      },
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Batch normalization is a modern technique to improve and speed up the training of deep neural networks (BatchNorm, Ioffe & Szegedy ICML'15, https://arxiv.org/abs/1502.03167). Each feature channel is normalized to have zero mean and unit variance across the spatial and mini-batch axes. To compensate for the lost degrees of freedom, extra scaling and bias parameters are introduced and learned. Mathematically, BatchNorm for a spatial feature map (e.g. the output of conv) can be written as:\n",
        "\n",
        "$$\n",
        "\\mu_d = \\mathbb{E}\\{x_{\\cdot \\cdot d}\\}, \\\\\n",
        "\\sigma_d = \\sqrt{\\operatorname{Var}\\{x_{\\cdot \\cdot d}\\}} \\\\\n",
        "z_{ijd} = \\gamma_d \\cdot \\frac{x_{ijd} - \\mu_d}{\\sigma_d} + \\beta_d,\\\\\n",
        "$$\n",
        "\n",
        "with the expectation and variance taken across both the data samples of the batch and the spatial dimensions.\n",
        "\n",
        "The $\\mu_d$ and $\\sigma_d$ values are computed on the actual mini-batch during training, but at test-time they are fixed, so that the prediction of the final system on a given sample does not depend on other samples in the mini-batch. To obtain the fixed values for test-time use, one needs to maintain moving statistics over the activations during training. This can be a bit tricky to implement from scratch, but luckily this is now implemented in all popular frameworks, including TensorFlow and Keras.\n",
        "\n",
        "When applying BatchNorm, it is not necessary to use biases in the previous convolutional layer. Why? Use the \"use_bias\" argument of `layers.Conv2D` accordingly.\n",
        "\n",
        "Furthermore, if the BatchNorm is followed by a linear or conv layer (with perhaps a ReLU in between), it is not necessary to use the $\\gamma_d$ factor in BatchNorm (it can be turned off as `layers.BatchNormalization(scale=False)`). Why? What about $\\beta_d$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VE2C9AhOV0W"
      },
      "source": [
        "The bias from the previous layer would be removed by the normalization.\n",
        "\n",
        "The following layer can perform the scaling, a composition of linear operations is still a linear operation. ReLU does not change this, as negative values stay negative even after scaling, if $\\gamma_d$ is positive. A negative $\\gamma_d$ would flip the sign, but that can be performed by the previous layer already. The bias term can not be omitted since it might negate selected values only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHDgDyBOV0W"
      },
      "source": [
        "Create a modified version of the previous model, where the `Conv2D` layers don't include the activation any more, and instead, insert a `layers.BatchNormalization()` and a `layers.Activation('relu')` layer after each conv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ4uSyk1OV0W",
        "outputId": "576c93a9-b991-43ff-c76e-53b45b2ff50f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 1.4015 - accuracy: 0.5101 - val_loss: 2.2832 - val_accuracy: 0.2608\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 1.0139 - accuracy: 0.6452 - val_loss: 1.1508 - val_accuracy: 0.6139\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.8905 - accuracy: 0.6902 - val_loss: 0.9533 - val_accuracy: 0.6751\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.8158 - accuracy: 0.7188 - val_loss: 1.0346 - val_accuracy: 0.6562\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.7486 - accuracy: 0.7427 - val_loss: 0.9796 - val_accuracy: 0.6712\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.7024 - accuracy: 0.7575 - val_loss: 0.9139 - val_accuracy: 0.7007\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.6611 - accuracy: 0.7703 - val_loss: 0.8907 - val_accuracy: 0.7065\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.6206 - accuracy: 0.7863 - val_loss: 0.8531 - val_accuracy: 0.7199\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.5853 - accuracy: 0.7967 - val_loss: 0.8777 - val_accuracy: 0.7090\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.5462 - accuracy: 0.8106 - val_loss: 0.8538 - val_accuracy: 0.7210\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.5195 - accuracy: 0.8212 - val_loss: 0.9174 - val_accuracy: 0.7035\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.4889 - accuracy: 0.8312 - val_loss: 0.8717 - val_accuracy: 0.7209\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.4604 - accuracy: 0.8413 - val_loss: 0.8684 - val_accuracy: 0.7299\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.4432 - accuracy: 0.8456 - val_loss: 0.8898 - val_accuracy: 0.7209\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.4165 - accuracy: 0.8579 - val_loss: 0.8985 - val_accuracy: 0.7263\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3973 - accuracy: 0.8625 - val_loss: 1.0158 - val_accuracy: 0.7062\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.3739 - accuracy: 0.8718 - val_loss: 0.8927 - val_accuracy: 0.7278\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.3598 - accuracy: 0.8745 - val_loss: 0.9733 - val_accuracy: 0.7164\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3353 - accuracy: 0.8853 - val_loss: 0.9170 - val_accuracy: 0.7231\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3189 - accuracy: 0.8919 - val_loss: 1.0046 - val_accuracy: 0.7191\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3052 - accuracy: 0.8947 - val_loss: 0.9674 - val_accuracy: 0.7303\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.2938 - accuracy: 0.8973 - val_loss: 0.9651 - val_accuracy: 0.7307\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.2739 - accuracy: 0.9065 - val_loss: 0.9930 - val_accuracy: 0.7244\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.2631 - accuracy: 0.9102 - val_loss: 1.0399 - val_accuracy: 0.7174\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.2454 - accuracy: 0.9157 - val_loss: 1.0792 - val_accuracy: 0.7103\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.2329 - accuracy: 0.9208 - val_loss: 1.0209 - val_accuracy: 0.7302\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.2211 - accuracy: 0.9252 - val_loss: 1.0261 - val_accuracy: 0.7322\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.2135 - accuracy: 0.9279 - val_loss: 1.1151 - val_accuracy: 0.7128\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1995 - accuracy: 0.9335 - val_loss: 1.0683 - val_accuracy: 0.7314\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.1903 - accuracy: 0.9378 - val_loss: 1.0635 - val_accuracy: 0.7254\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.1856 - accuracy: 0.9380 - val_loss: 1.1201 - val_accuracy: 0.7311\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.1775 - accuracy: 0.9403 - val_loss: 1.1477 - val_accuracy: 0.7210\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1665 - accuracy: 0.9456 - val_loss: 1.1146 - val_accuracy: 0.7253\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1519 - accuracy: 0.9517 - val_loss: 1.1290 - val_accuracy: 0.7291\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1494 - accuracy: 0.9522 - val_loss: 1.2551 - val_accuracy: 0.7142\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.1370 - accuracy: 0.9570 - val_loss: 1.1807 - val_accuracy: 0.7228\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1417 - accuracy: 0.9530 - val_loss: 1.2401 - val_accuracy: 0.7119\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1284 - accuracy: 0.9610 - val_loss: 1.2661 - val_accuracy: 0.7185\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1275 - accuracy: 0.9595 - val_loss: 1.2085 - val_accuracy: 0.7291\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1175 - accuracy: 0.9641 - val_loss: 1.3259 - val_accuracy: 0.7120\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1156 - accuracy: 0.9641 - val_loss: 1.2690 - val_accuracy: 0.7241\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.1093 - accuracy: 0.9658 - val_loss: 1.2595 - val_accuracy: 0.7256\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1057 - accuracy: 0.9677 - val_loss: 1.3537 - val_accuracy: 0.7175\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1039 - accuracy: 0.9675 - val_loss: 1.3480 - val_accuracy: 0.7166\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0898 - accuracy: 0.9737 - val_loss: 1.4205 - val_accuracy: 0.7093\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1008 - accuracy: 0.9684 - val_loss: 1.4264 - val_accuracy: 0.7124\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0887 - accuracy: 0.9747 - val_loss: 1.3861 - val_accuracy: 0.7197\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0877 - accuracy: 0.9735 - val_loss: 1.4577 - val_accuracy: 0.7202\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0882 - accuracy: 0.9733 - val_loss: 1.4709 - val_accuracy: 0.7165\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0774 - accuracy: 0.9780 - val_loss: 1.4335 - val_accuracy: 0.7166\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0795 - accuracy: 0.9765 - val_loss: 1.6222 - val_accuracy: 0.6977\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0779 - accuracy: 0.9769 - val_loss: 1.4603 - val_accuracy: 0.7144\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0657 - accuracy: 0.9829 - val_loss: 1.4948 - val_accuracy: 0.7192\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0932 - accuracy: 0.9703 - val_loss: 1.4837 - val_accuracy: 0.7187\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0636 - accuracy: 0.9831 - val_loss: 1.5165 - val_accuracy: 0.7123\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0629 - accuracy: 0.9831 - val_loss: 1.5208 - val_accuracy: 0.7165\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0575 - accuracy: 0.9852 - val_loss: 1.4858 - val_accuracy: 0.7200\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0658 - accuracy: 0.9809 - val_loss: 1.5720 - val_accuracy: 0.7186\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0572 - accuracy: 0.9845 - val_loss: 1.6239 - val_accuracy: 0.7075\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0625 - accuracy: 0.9821 - val_loss: 1.5999 - val_accuracy: 0.7182\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 1.5918 - val_accuracy: 0.7190\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0572 - accuracy: 0.9834 - val_loss: 1.6261 - val_accuracy: 0.7140\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 1.7266 - val_accuracy: 0.7115\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0655 - accuracy: 0.9799 - val_loss: 1.7852 - val_accuracy: 0.7080\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0513 - accuracy: 0.9861 - val_loss: 1.6465 - val_accuracy: 0.7101\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0459 - accuracy: 0.9881 - val_loss: 1.6818 - val_accuracy: 0.7186\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.0478 - accuracy: 0.9868 - val_loss: 1.7152 - val_accuracy: 0.7190\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0610 - accuracy: 0.9814 - val_loss: 1.7371 - val_accuracy: 0.7096\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0405 - accuracy: 0.9898 - val_loss: 1.7369 - val_accuracy: 0.7133\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0327 - accuracy: 0.9934 - val_loss: 1.7096 - val_accuracy: 0.7122\n"
          ]
        }
      ],
      "source": [
        "cnn_batchnorm = models.Sequential([\n",
        "    layers.Conv2D(64, (3, 3), use_bias=False,\n",
        "                  padding='same', input_shape=image_shape),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D((2, 2), (2,2)),\n",
        "    layers.Conv2D(64, (3, 3), use_bias=False,\n",
        "                  padding='same'),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D((2, 2), (2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='cnn_batchnorm')\n",
        "\n",
        "train_model(cnn_batchnorm, optimizer=optimizers.Adam, learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiqPWFMROV0W"
      },
      "source": [
        "## Strided Convolutions\n",
        "\n",
        "Max-pooling is a popular technique for reducing the spatial dimensionality\n",
        "of the outputs from conv layers. Another way to reduce dimensionality is striding. For an argument why this may be similarly effective, see [Springenberg et al., ICLRW'15](https://arxiv.org/pdf/1412.6806.pdf).\n",
        "\n",
        "Now create a model using the same architecture as before, with the difference of\n",
        "removing the max-pooling layers and increasing the stride parameter of the conv layers to $2 \\times 2$ in the spatial dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iCfsGRLEOV0W",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacc04b6-d2f5-41cc-f398-9288a967cf37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 1.4282 - accuracy: 0.4935 - val_loss: 2.4608 - val_accuracy: 0.2640\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.0705 - accuracy: 0.6223 - val_loss: 1.0489 - val_accuracy: 0.6268\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9321 - accuracy: 0.6725 - val_loss: 1.0057 - val_accuracy: 0.6526\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8424 - accuracy: 0.7066 - val_loss: 0.9873 - val_accuracy: 0.6616\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.7756 - accuracy: 0.7310 - val_loss: 0.9836 - val_accuracy: 0.6635\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.7235 - accuracy: 0.7517 - val_loss: 0.9742 - val_accuracy: 0.6733\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.6761 - accuracy: 0.7671 - val_loss: 0.9849 - val_accuracy: 0.6696\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.6387 - accuracy: 0.7770 - val_loss: 0.9658 - val_accuracy: 0.6796\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5875 - accuracy: 0.7981 - val_loss: 1.0001 - val_accuracy: 0.6753\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5543 - accuracy: 0.8099 - val_loss: 0.9793 - val_accuracy: 0.6786\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5182 - accuracy: 0.8216 - val_loss: 1.0264 - val_accuracy: 0.6734\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.4952 - accuracy: 0.8286 - val_loss: 1.0475 - val_accuracy: 0.6785\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.4637 - accuracy: 0.8416 - val_loss: 1.0729 - val_accuracy: 0.6687\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.4368 - accuracy: 0.8504 - val_loss: 1.0644 - val_accuracy: 0.6774\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.4165 - accuracy: 0.8572 - val_loss: 1.1150 - val_accuracy: 0.6680\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.3926 - accuracy: 0.8656 - val_loss: 1.1080 - val_accuracy: 0.6732\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.3670 - accuracy: 0.8753 - val_loss: 1.1582 - val_accuracy: 0.6633\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.3522 - accuracy: 0.8804 - val_loss: 1.1410 - val_accuracy: 0.6731\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.3313 - accuracy: 0.8881 - val_loss: 1.1991 - val_accuracy: 0.6679\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.3180 - accuracy: 0.8925 - val_loss: 1.1990 - val_accuracy: 0.6666\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2974 - accuracy: 0.9010 - val_loss: 1.2215 - val_accuracy: 0.6680\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2870 - accuracy: 0.9037 - val_loss: 1.2592 - val_accuracy: 0.6694\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.2651 - accuracy: 0.9124 - val_loss: 1.2984 - val_accuracy: 0.6622\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2529 - accuracy: 0.9169 - val_loss: 1.3172 - val_accuracy: 0.6645\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2413 - accuracy: 0.9210 - val_loss: 1.3692 - val_accuracy: 0.6587\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.2289 - accuracy: 0.9246 - val_loss: 1.3580 - val_accuracy: 0.6603\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.2197 - accuracy: 0.9291 - val_loss: 1.3681 - val_accuracy: 0.6666\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2096 - accuracy: 0.9323 - val_loss: 1.4192 - val_accuracy: 0.6641\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1941 - accuracy: 0.9395 - val_loss: 1.4553 - val_accuracy: 0.6569\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1853 - accuracy: 0.9409 - val_loss: 1.4510 - val_accuracy: 0.6680\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1768 - accuracy: 0.9455 - val_loss: 1.5257 - val_accuracy: 0.6535\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1757 - accuracy: 0.9435 - val_loss: 1.5651 - val_accuracy: 0.6529\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1618 - accuracy: 0.9506 - val_loss: 1.5519 - val_accuracy: 0.6558\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1510 - accuracy: 0.9555 - val_loss: 1.5855 - val_accuracy: 0.6562\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1503 - accuracy: 0.9540 - val_loss: 1.6521 - val_accuracy: 0.6533\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1470 - accuracy: 0.9548 - val_loss: 1.6405 - val_accuracy: 0.6526\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1338 - accuracy: 0.9605 - val_loss: 1.6326 - val_accuracy: 0.6549\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1304 - accuracy: 0.9607 - val_loss: 1.6767 - val_accuracy: 0.6535\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1252 - accuracy: 0.9627 - val_loss: 1.7191 - val_accuracy: 0.6453\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1186 - accuracy: 0.9656 - val_loss: 1.7475 - val_accuracy: 0.6541\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1131 - accuracy: 0.9681 - val_loss: 1.7897 - val_accuracy: 0.6519\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1085 - accuracy: 0.9684 - val_loss: 1.8022 - val_accuracy: 0.6478\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1103 - accuracy: 0.9680 - val_loss: 1.8298 - val_accuracy: 0.6540\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1050 - accuracy: 0.9696 - val_loss: 1.8410 - val_accuracy: 0.6465\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1010 - accuracy: 0.9709 - val_loss: 1.8692 - val_accuracy: 0.6493\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0953 - accuracy: 0.9740 - val_loss: 1.8999 - val_accuracy: 0.6435\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0847 - accuracy: 0.9778 - val_loss: 1.9745 - val_accuracy: 0.6376\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0871 - accuracy: 0.9765 - val_loss: 1.9767 - val_accuracy: 0.6475\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0816 - accuracy: 0.9779 - val_loss: 1.9677 - val_accuracy: 0.6411\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0809 - accuracy: 0.9786 - val_loss: 1.9586 - val_accuracy: 0.6502\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0815 - accuracy: 0.9773 - val_loss: 2.0187 - val_accuracy: 0.6432\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0774 - accuracy: 0.9787 - val_loss: 2.0412 - val_accuracy: 0.6443\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0716 - accuracy: 0.9815 - val_loss: 2.0661 - val_accuracy: 0.6473\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0761 - accuracy: 0.9792 - val_loss: 2.0646 - val_accuracy: 0.6470\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0678 - accuracy: 0.9824 - val_loss: 2.0826 - val_accuracy: 0.6477\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0689 - accuracy: 0.9817 - val_loss: 2.1202 - val_accuracy: 0.6482\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0629 - accuracy: 0.9843 - val_loss: 2.1328 - val_accuracy: 0.6441\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0549 - accuracy: 0.9881 - val_loss: 2.1510 - val_accuracy: 0.6490\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0640 - accuracy: 0.9830 - val_loss: 2.1991 - val_accuracy: 0.6398\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0729 - accuracy: 0.9792 - val_loss: 2.1954 - val_accuracy: 0.6457\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0549 - accuracy: 0.9870 - val_loss: 2.2895 - val_accuracy: 0.6391\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0540 - accuracy: 0.9864 - val_loss: 2.3151 - val_accuracy: 0.6376\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0647 - accuracy: 0.9817 - val_loss: 2.2703 - val_accuracy: 0.6365\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0558 - accuracy: 0.9854 - val_loss: 2.2292 - val_accuracy: 0.6494\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0442 - accuracy: 0.9905 - val_loss: 2.2989 - val_accuracy: 0.6492\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0530 - accuracy: 0.9861 - val_loss: 2.3920 - val_accuracy: 0.6411\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0593 - accuracy: 0.9823 - val_loss: 2.3208 - val_accuracy: 0.6467\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0543 - accuracy: 0.9852 - val_loss: 2.3448 - val_accuracy: 0.6500\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 2.3944 - val_accuracy: 0.6399\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0456 - accuracy: 0.9888 - val_loss: 2.3786 - val_accuracy: 0.6437\n"
          ]
        }
      ],
      "source": [
        "cnn_strides = models.Sequential([\n",
        "    layers.Conv2D(64, 3, strides=2, use_bias=False, \n",
        "                  padding='same', input_shape=image_shape),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, strides=2, use_bias=False, padding='same'),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='cnn_strides')\n",
        "\n",
        "train_model(cnn_strides, optimizer=optimizers.Adam, learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wXljy1SOV0W"
      },
      "source": [
        "Q: What differences do you notice when training this new network?\n",
        "What is a clear advantage of using strides?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr3n3KiOOV0W"
      },
      "source": [
        "Training is faster. For pooling with size 2 all four features have to be computed to get the max/mean of each window. For striding only a single feature per window has to be computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ2DjQ2POV0W"
      },
      "source": [
        "## Global Pooling\n",
        "\n",
        "The above network ends in a `Flatten` layer followed by a `Dense` layer, in which the number of weights depends on the input size. This means that testing can only be performed on the exact same image size. Several architectures employ a (spatial) **global average pooling layer** to produce of vector of fixed size describing the whole image, instead of flattening.\n",
        "\n",
        "For this to work well, the units before the average pooling need to have a large enough receptive field. Therefore, compared with the previous model, remove the `Flatten` layer and instead add a third Conv-BatchNorm-ReLU combination, followed by a `layers.GlobalAveragePooling2D()` layer (before the final `Dense` layer).\n",
        "\n",
        "Train it and see if it reaches similar accuracy to the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OnehXDJDOV0W",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1577a81-8fd7-455d-bf2d-596ffd1e0d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 6s 14ms/step - loss: 1.5724 - accuracy: 0.4423 - val_loss: 2.4251 - val_accuracy: 0.2206\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 1.2746 - accuracy: 0.5485 - val_loss: 1.2509 - val_accuracy: 0.5478\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 1.1628 - accuracy: 0.5907 - val_loss: 1.2760 - val_accuracy: 0.5496\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 1.0838 - accuracy: 0.6181 - val_loss: 1.1859 - val_accuracy: 0.5765\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 1.0283 - accuracy: 0.6389 - val_loss: 1.0780 - val_accuracy: 0.6098\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.9850 - accuracy: 0.6536 - val_loss: 1.0718 - val_accuracy: 0.6182\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.9435 - accuracy: 0.6706 - val_loss: 1.0493 - val_accuracy: 0.6206\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.9166 - accuracy: 0.6780 - val_loss: 1.0481 - val_accuracy: 0.6217\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.8888 - accuracy: 0.6901 - val_loss: 1.1015 - val_accuracy: 0.6094\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.8642 - accuracy: 0.6989 - val_loss: 1.0188 - val_accuracy: 0.6432\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.8437 - accuracy: 0.7059 - val_loss: 1.0591 - val_accuracy: 0.6314\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.8245 - accuracy: 0.7120 - val_loss: 0.9599 - val_accuracy: 0.6597\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.8030 - accuracy: 0.7195 - val_loss: 0.9422 - val_accuracy: 0.6634\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.7891 - accuracy: 0.7239 - val_loss: 0.9209 - val_accuracy: 0.6712\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.7686 - accuracy: 0.7322 - val_loss: 0.9529 - val_accuracy: 0.6640\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.7563 - accuracy: 0.7350 - val_loss: 0.9544 - val_accuracy: 0.6645\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.7440 - accuracy: 0.7400 - val_loss: 0.9595 - val_accuracy: 0.6675\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.7245 - accuracy: 0.7485 - val_loss: 0.9219 - val_accuracy: 0.6746\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.7146 - accuracy: 0.7513 - val_loss: 1.0529 - val_accuracy: 0.6297\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.7019 - accuracy: 0.7552 - val_loss: 0.9248 - val_accuracy: 0.6743\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.6899 - accuracy: 0.7587 - val_loss: 0.9656 - val_accuracy: 0.6717\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.6787 - accuracy: 0.7659 - val_loss: 0.9197 - val_accuracy: 0.6800\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.6660 - accuracy: 0.7685 - val_loss: 0.9087 - val_accuracy: 0.6796\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.6608 - accuracy: 0.7715 - val_loss: 0.8630 - val_accuracy: 0.7020\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6499 - accuracy: 0.7770 - val_loss: 0.9832 - val_accuracy: 0.6622\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6353 - accuracy: 0.7812 - val_loss: 0.9435 - val_accuracy: 0.6768\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.6264 - accuracy: 0.7834 - val_loss: 0.9026 - val_accuracy: 0.6907\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6204 - accuracy: 0.7851 - val_loss: 1.0149 - val_accuracy: 0.6567\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6133 - accuracy: 0.7882 - val_loss: 0.8824 - val_accuracy: 0.6980\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6009 - accuracy: 0.7933 - val_loss: 1.0069 - val_accuracy: 0.6654\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5915 - accuracy: 0.7962 - val_loss: 0.9048 - val_accuracy: 0.6899\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5828 - accuracy: 0.7986 - val_loss: 1.0224 - val_accuracy: 0.6566\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5720 - accuracy: 0.8043 - val_loss: 0.8676 - val_accuracy: 0.6982\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5684 - accuracy: 0.8048 - val_loss: 0.9544 - val_accuracy: 0.6778\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5616 - accuracy: 0.8069 - val_loss: 0.8727 - val_accuracy: 0.6983\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5482 - accuracy: 0.8129 - val_loss: 0.9019 - val_accuracy: 0.6860\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5460 - accuracy: 0.8119 - val_loss: 0.8710 - val_accuracy: 0.6992\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5371 - accuracy: 0.8164 - val_loss: 0.9514 - val_accuracy: 0.6737\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5291 - accuracy: 0.8193 - val_loss: 0.8974 - val_accuracy: 0.6987\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5241 - accuracy: 0.8219 - val_loss: 0.9498 - val_accuracy: 0.6768\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5173 - accuracy: 0.8214 - val_loss: 1.0435 - val_accuracy: 0.6594\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.5095 - accuracy: 0.8258 - val_loss: 0.9200 - val_accuracy: 0.6931\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5035 - accuracy: 0.8285 - val_loss: 0.9529 - val_accuracy: 0.6778\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4987 - accuracy: 0.8306 - val_loss: 0.9654 - val_accuracy: 0.6805\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4906 - accuracy: 0.8325 - val_loss: 0.9601 - val_accuracy: 0.6734\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4873 - accuracy: 0.8333 - val_loss: 0.8777 - val_accuracy: 0.7030\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4757 - accuracy: 0.8382 - val_loss: 0.9384 - val_accuracy: 0.6893\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4754 - accuracy: 0.8390 - val_loss: 0.9297 - val_accuracy: 0.6911\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4667 - accuracy: 0.8403 - val_loss: 0.9565 - val_accuracy: 0.6942\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4655 - accuracy: 0.8423 - val_loss: 0.9036 - val_accuracy: 0.7001\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4555 - accuracy: 0.8461 - val_loss: 0.9329 - val_accuracy: 0.6963\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4522 - accuracy: 0.8451 - val_loss: 0.9020 - val_accuracy: 0.6990\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4473 - accuracy: 0.8486 - val_loss: 1.0420 - val_accuracy: 0.6683\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4428 - accuracy: 0.8498 - val_loss: 0.9471 - val_accuracy: 0.6935\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4376 - accuracy: 0.8525 - val_loss: 0.9075 - val_accuracy: 0.7030\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4319 - accuracy: 0.8541 - val_loss: 0.9107 - val_accuracy: 0.6999\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4285 - accuracy: 0.8551 - val_loss: 0.9238 - val_accuracy: 0.7011\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4200 - accuracy: 0.8581 - val_loss: 1.0988 - val_accuracy: 0.6566\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4166 - accuracy: 0.8601 - val_loss: 0.8931 - val_accuracy: 0.7152\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4119 - accuracy: 0.8610 - val_loss: 0.9760 - val_accuracy: 0.6843\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.4058 - accuracy: 0.8632 - val_loss: 0.9158 - val_accuracy: 0.7082\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.4033 - accuracy: 0.8640 - val_loss: 0.9949 - val_accuracy: 0.6946\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3984 - accuracy: 0.8661 - val_loss: 0.9231 - val_accuracy: 0.7064\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3909 - accuracy: 0.8693 - val_loss: 0.9945 - val_accuracy: 0.6853\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.3904 - accuracy: 0.8691 - val_loss: 1.0032 - val_accuracy: 0.6865\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3882 - accuracy: 0.8686 - val_loss: 0.8897 - val_accuracy: 0.7150\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3800 - accuracy: 0.8738 - val_loss: 0.8801 - val_accuracy: 0.7151\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3761 - accuracy: 0.8731 - val_loss: 1.0353 - val_accuracy: 0.6742\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3776 - accuracy: 0.8734 - val_loss: 1.0015 - val_accuracy: 0.6889\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3669 - accuracy: 0.8786 - val_loss: 0.9857 - val_accuracy: 0.6896\n"
          ]
        }
      ],
      "source": [
        "cnn_global_pool = models.Sequential([\n",
        "    layers.Conv2D(64, 3, 2, padding='same', use_bias=False),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, 2, padding='same', use_bias=False),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
        "    layers.BatchNormalization(scale=False),\n",
        "    layers.Activation('relu'),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(10, activation='softmax')],\n",
        "    name='cnn_global_pool')\n",
        "\n",
        "train_model(cnn_global_pool, optimizer=optimizers.Adam, learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPOMVAhfOV0X"
      },
      "source": [
        "Q: Which network has more parameters, this or the previous one?\n",
        "\n",
        "Q: What is the size of the receptive field of the units in the layer directly before the global average pooling? (Remember: the receptive field of a particular unit (neuron) is the area of the *input image* that can influence the activation of this given unit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0YxEWFOV0X"
      },
      "source": [
        "The first layers are the same in both models, so we only need to compare the last layers. The previous model has $40970$ parameters in the last dense layer. The new model has a convolutional layer with $36864$ weights, followed by a bias term (in the BatchNorm) with $64$ parameters. After the global pooling, $64$ features are left, so the final dense layer has $640$ weights and a bias of size $10$. $37578$ parameters in total.\n",
        "\n",
        "The first convolution has a receptive field of $3\\times3$. This is applied with stride 2, so the second convolution has a receptive field of size $7\\times7$. This is again applied with stride 2, thus the final layer has stride $15\\times15$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/tensorboard_logs_cnn01_01.zip /content/tensorboard_logs\n",
        "from google.colab import files\n",
        "files.download(\"/content/tensorboard_logs_cnn01_01.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "UBIxKh6VkzSb",
        "outputId": "1fc13e93-66b7-49e7-ae94-97aca064ad7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/tensorboard_logs/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_20220403-204025/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_20220403-204025/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_20220403-204025/validation/events.out.tfevents.1649018439.31934f8d3058.72.7.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/cnn_20220403-204025/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_20220403-204025/train/events.out.tfevents.1649018426.31934f8d3058.72.6.v2 (deflated 83%)\n",
            "  adding: content/tensorboard_logs/cnn_batchnorm_20220403-204733/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_batchnorm_20220403-204733/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_batchnorm_20220403-204733/validation/events.out.tfevents.1649018861.31934f8d3058.72.9.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/cnn_batchnorm_20220403-204733/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_batchnorm_20220403-204733/train/events.out.tfevents.1649018854.31934f8d3058.72.8.v2 (deflated 85%)\n",
            "  adding: content/tensorboard_logs/cnn_strides_20220403-205557/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_strides_20220403-205557/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_strides_20220403-205557/validation/events.out.tfevents.1649019362.31934f8d3058.72.11.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/cnn_strides_20220403-205557/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_strides_20220403-205557/train/events.out.tfevents.1649019358.31934f8d3058.72.10.v2 (deflated 85%)\n",
            "  adding: content/tensorboard_logs/linear_adam_20220403-203206/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/linear_adam_20220403-203206/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/linear_adam_20220403-203206/validation/events.out.tfevents.1649017931.31934f8d3058.72.1.v2 (deflated 79%)\n",
            "  adding: content/tensorboard_logs/linear_adam_20220403-203206/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/linear_adam_20220403-203206/train/events.out.tfevents.1649017927.31934f8d3058.72.0.v2 (deflated 81%)\n",
            "  adding: content/tensorboard_logs/tanh_mlp_20220403-203430/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/tanh_mlp_20220403-203430/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/tanh_mlp_20220403-203430/validation/events.out.tfevents.1649018074.31934f8d3058.72.3.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/tanh_mlp_20220403-203430/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/tanh_mlp_20220403-203430/train/events.out.tfevents.1649018071.31934f8d3058.72.2.v2 (deflated 82%)\n",
            "  adding: content/tensorboard_logs/cnn_global_pool_20220403-210120/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_global_pool_20220403-210120/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_global_pool_20220403-210120/validation/events.out.tfevents.1649019687.31934f8d3058.72.13.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/cnn_global_pool_20220403-210120/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/cnn_global_pool_20220403-210120/train/events.out.tfevents.1649019681.31934f8d3058.72.12.v2 (deflated 87%)\n",
            "  adding: content/tensorboard_logs/relu_mlp_20220403-203732/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/relu_mlp_20220403-203732/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/relu_mlp_20220403-203732/validation/events.out.tfevents.1649018255.31934f8d3058.72.5.v2 (deflated 79%)\n",
            "  adding: content/tensorboard_logs/relu_mlp_20220403-203732/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/relu_mlp_20220403-203732/train/events.out.tfevents.1649018253.31934f8d3058.72.4.v2 (deflated 82%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e02699d9-4b2c-469d-8ccc-96a3bfa50ea6\", \"tensorboard_logs_cnn01_01.zip\", 100313)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN - ReLU, BatchNorm, Global Pooling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}