{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN - ResNets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2lCjuewWje9Q87PpA7BNu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra72/Computer-Vision/blob/main/CNN_ResNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inhl_PiHhuhX",
        "outputId": "2496cf0b-bff0-4957-9336-5dbb59df6243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "log_root = 'tensorboard_logs'\n",
        "\n",
        "%matplotlib inline\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import cv2\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.regularizers as regularizers\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import tensorflow.keras.callbacks as callbacks\n",
        "import tensorflow.keras.initializers as initializers\n",
        "import tensorflow.keras.preprocessing.image as kerasimage\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just an image plotting function\n",
        "def plot_multiple(images, titles=None, colormap='gray',\n",
        "                  max_columns=np.inf, imwidth=4, imheight=4, share_axes=False):\n",
        "    \"\"\"Plot multiple images as subplots on a grid.\"\"\"\n",
        "    if titles is None:\n",
        "        titles = [''] *len(images)\n",
        "    assert len(images) == len(titles)\n",
        "    n_images = len(images)\n",
        "    n_cols = min(max_columns, n_images)\n",
        "    n_rows = int(np.ceil(n_images / n_cols))\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
        "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
        "\n",
        "    axes = axes.flat\n",
        "    # Hide subplots without content\n",
        "    for ax in axes[n_images:]:\n",
        "        ax.axis('off')\n",
        "        \n",
        "    if not isinstance(colormap, (list,tuple)):\n",
        "        colormaps = [colormap]*n_images\n",
        "    else:\n",
        "        colormaps = colormap\n",
        "\n",
        "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
        "        ax.imshow(image, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        \n",
        "    fig.tight_layout()"
      ],
      "metadata": {
        "id": "OMyEHIStiSLD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(im_train, y_train), (im_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize to 0-1 range and subtract mean of training pixels\n",
        "### BEGIN SOLUTION\n",
        "im_train = im_train / 255\n",
        "im_test = im_test / 255\n",
        "\n",
        "mean_training_pixel = np.mean(im_train, axis=(0,1,2))\n",
        "x_train = im_train - mean_training_pixel\n",
        "x_test = im_test - mean_training_pixel\n",
        "### END SOLUTION\n",
        "\n",
        "image_shape = x_train[0].shape\n",
        "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X8KPWExhzFp",
        "outputId": "b3653658-3a3b-417b-dadf-f01cc7ab1e26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X__Ls0IoOV0X"
      },
      "source": [
        "## Residual Networks\n",
        "\n",
        "ResNet is a more modern architecture, introduced by He et al. in 2015 (published in 2016: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) and is still popular today.\n",
        "\n",
        "It consists of blocks like the following:\n",
        "\n",
        "![ResNet Block](resnet_block.png)\n",
        "\n",
        "Each of these so-called *residual blocks* only have to predict a *residual* (in plain words: the \"rest\", the \"leftover\") that will be added on top of its input.\n",
        "In other words, the block outputs how much each feature needs to be changed in order to enhance the representation compared to the previous block.\n",
        "\n",
        "There are several ways to combine residual blocks into *residual networks* (ResNets). In the following, we consider ResNet-v1, as used for the CIFAR-10 benchmark in the original ResNet paper (it is simpler compared to the full model that they used for the much larger ImageNet benchmark).\n",
        "\n",
        "Section 4.2. of the paper describes this architecture as follows: \"*The first layer is 3×3 convolutions. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [...] When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases.*\"\n",
        "\n",
        "Further, they use L2 regularization for training (a standard tool to combat overfitting). This penalizes weights with large magnitude by adding an additional term to the cost function, besides the cross-entropy. The overall function to optimize becomes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{CE} + \\frac{\\lambda}{2} \\sum_{w\\in\\text{weights}} w^2,\n",
        "$$\n",
        "\n",
        "and in this paper $\\lambda=10^{-4}$.\n",
        "\n",
        "In the previous parts of this exercise we have already seen every major component we need to build this thing. However, ResNet is not a pure sequential architecture due to the skip connections. This means we cannot use `models.Sequential`. Luckily, Keras also offers a functional API. Look below to understand how this API works and fill in the missing pieces to make a ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VWI3I-ZTOV0X"
      },
      "outputs": [],
      "source": [
        "def resnet(num_layers=56):\n",
        "    if (num_layers - 2) % 6 != 0:\n",
        "        raise ValueError('n_layers should be 6n+2 (eg 20, 32, 44, 56)')\n",
        "    n = (num_layers - 2) // 6\n",
        "        \n",
        "    inputs = layers.Input(shape=image_shape)\n",
        "    \n",
        "    # First layer\n",
        "    x = layers.Conv2D(16, 3, use_bias=False, \n",
        "        kernel_regularizer=regularizers.l2(1e-4),\n",
        "        padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    x = layers.BatchNormalization(scale=False)(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    \n",
        "    # `resnet_block` function call in loops to stack ResNet blocks as per refernce above.\n",
        "    for i_block in range(n):\n",
        "        x = resnet_block(x, 16, strides=1)\n",
        "        \n",
        "    for i_block in range(n):\n",
        "        x = resnet_block(x, 32, strides=2 if i_block==0 else 1)\n",
        "        \n",
        "    for i_block in range(n):\n",
        "        x = resnet_block(x, 64, strides=2 if i_block==0 else 1)\n",
        "\n",
        "    # Global pooling and classifier on top\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(10, activation='softmax',\n",
        "            kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    return models.Model(inputs=inputs, outputs=outputs, name=f'resnet{num_layers}')\n",
        "\n",
        "def resnet_block(x, n_channels_out, strides=1):\n",
        "    # First conv\n",
        "    f = layers.Conv2D(n_channels_out, 3, strides, use_bias=False,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            padding='same', kernel_initializer='he_normal')(x)\n",
        "    f = layers.BatchNormalization(scale=False)(f)\n",
        "    f = layers.Activation('relu')(f)\n",
        "\n",
        "    # Second conv\n",
        "    f = layers.Conv2D(n_channels_out, 3, use_bias=False,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            padding='same', kernel_initializer='he_normal')(f)\n",
        "    f = layers.BatchNormalization(scale=False)(f)\n",
        "    \n",
        "    ## Shortcut Connection:\n",
        "    # If feature channel counts differ between input and output,\n",
        "    # zero padding is used to match the depths.\n",
        "    # It is implemented by a Conv2D with fixed weights.\n",
        "    n_channels_in = x.shape[-1]\n",
        "    if n_channels_in != n_channels_out:\n",
        "        # Fixed weights, np.eye returns a matrix with 1s along the \n",
        "        # main diagonal and zeros elsewhere.\n",
        "        identity_weights = np.eye(n_channels_in, n_channels_out, dtype=np.float32)\n",
        "        layer = layers.Conv2D(\n",
        "            n_channels_out, kernel_size=1, strides=strides, use_bias=False, \n",
        "            kernel_initializer=initializers.Constant(value=identity_weights))\n",
        "        # Weight is not learnt\n",
        "        layer.trainable = False\n",
        "        x = layer(x)\n",
        "       \n",
        "    # the shortcut connection is added to the residual.\n",
        "    x = layers.add([x, f])\n",
        "    return layers.Activation('relu')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgv3zBmaOV0X"
      },
      "source": [
        "## Learning Rate Decay and Data Augmentation - Our Final Model\n",
        "\n",
        "Learning rate decay reduces the learning rate as the training progresses. It can be implemented as a Keras callback as shown below.\n",
        "\n",
        "If you have a good GPU or a lot of time, train ResNet-56 on the CIFAR-10 dataset for 75 epochs. As a rough idea, it will take about one hour with a good GPU, but on a CPU it could take a day or two. If that's too long, train a smaller ResNet, wih `num_layers`=14 or 20, or do fewer epochs.\n",
        "\n",
        "To add data augmentation (e.g. random translation or rotation of the input images), look up the documentation for the `ImageDataGenerator` class. The ResNet model presented in the original paper was trained with random translations of $\\pm$ 4 px.\n",
        "\n",
        "Note: `model.fit` with generator input seems to only work when the `y` targets are provided as one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U9LHEyYrOV0X",
        "outputId": "cbc5f1df-d2ee-4fd3-81b9-310805bac53e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 104s 224ms/step - loss: 1.8783 - accuracy: 0.4769 - val_loss: 2.1403 - val_accuracy: 0.4257 - lr: 0.0010\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 1.3763 - accuracy: 0.6414 - val_loss: 1.3962 - val_accuracy: 0.6396 - lr: 0.0010\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 1.1502 - accuracy: 0.7179 - val_loss: 1.2874 - val_accuracy: 0.6610 - lr: 0.0010\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 1.0002 - accuracy: 0.7668 - val_loss: 1.5843 - val_accuracy: 0.6105 - lr: 0.0010\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.8919 - accuracy: 0.7995 - val_loss: 1.5978 - val_accuracy: 0.6290 - lr: 0.0010\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 85s 217ms/step - loss: 0.8147 - accuracy: 0.8227 - val_loss: 1.3014 - val_accuracy: 0.6908 - lr: 0.0010\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.7464 - accuracy: 0.8442 - val_loss: 1.6155 - val_accuracy: 0.6404 - lr: 0.0010\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6856 - accuracy: 0.8659 - val_loss: 1.1323 - val_accuracy: 0.7387 - lr: 0.0010\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.6371 - accuracy: 0.8791 - val_loss: 1.1327 - val_accuracy: 0.7476 - lr: 0.0010\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.5949 - accuracy: 0.8918 - val_loss: 1.4111 - val_accuracy: 0.6917 - lr: 0.0010\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.5619 - accuracy: 0.9037 - val_loss: 1.9230 - val_accuracy: 0.6292 - lr: 0.0010\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.5331 - accuracy: 0.9141 - val_loss: 2.1030 - val_accuracy: 0.6044 - lr: 0.0010\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.5033 - accuracy: 0.9247 - val_loss: 1.4644 - val_accuracy: 0.6849 - lr: 0.0010\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4869 - accuracy: 0.9322 - val_loss: 1.4184 - val_accuracy: 0.7035 - lr: 0.0010\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.4716 - accuracy: 0.9360 - val_loss: 1.3200 - val_accuracy: 0.7388 - lr: 0.0010\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4581 - accuracy: 0.9421 - val_loss: 1.7284 - val_accuracy: 0.6863 - lr: 0.0010\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.4462 - accuracy: 0.9454 - val_loss: 1.8504 - val_accuracy: 0.6636 - lr: 0.0010\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4407 - accuracy: 0.9487 - val_loss: 1.3707 - val_accuracy: 0.7442 - lr: 0.0010\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4291 - accuracy: 0.9536 - val_loss: 1.3396 - val_accuracy: 0.7654 - lr: 0.0010\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4215 - accuracy: 0.9545 - val_loss: 1.4197 - val_accuracy: 0.7359 - lr: 0.0010\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.4235 - accuracy: 0.9546 - val_loss: 1.4441 - val_accuracy: 0.7114 - lr: 0.0010\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.4109 - accuracy: 0.9577 - val_loss: 1.9041 - val_accuracy: 0.6616 - lr: 0.0010\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4090 - accuracy: 0.9588 - val_loss: 1.3616 - val_accuracy: 0.7597 - lr: 0.0010\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4084 - accuracy: 0.9605 - val_loss: 1.8906 - val_accuracy: 0.6709 - lr: 0.0010\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3981 - accuracy: 0.9627 - val_loss: 1.9625 - val_accuracy: 0.6601 - lr: 0.0010\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4017 - accuracy: 0.9604 - val_loss: 2.5956 - val_accuracy: 0.6099 - lr: 0.0010\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3972 - accuracy: 0.9622 - val_loss: 1.5100 - val_accuracy: 0.7440 - lr: 0.0010\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3915 - accuracy: 0.9632 - val_loss: 1.5479 - val_accuracy: 0.7154 - lr: 0.0010\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3925 - accuracy: 0.9639 - val_loss: 2.2226 - val_accuracy: 0.6437 - lr: 0.0010\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.3811 - accuracy: 0.9671 - val_loss: 1.8134 - val_accuracy: 0.7082 - lr: 0.0010\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3875 - accuracy: 0.9643 - val_loss: 1.8854 - val_accuracy: 0.7101 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3800 - accuracy: 0.9674 - val_loss: 1.4556 - val_accuracy: 0.7284 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3841 - accuracy: 0.9648 - val_loss: 1.6191 - val_accuracy: 0.7165 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.3852 - accuracy: 0.9648 - val_loss: 1.2103 - val_accuracy: 0.7841 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3757 - accuracy: 0.9681 - val_loss: 1.7115 - val_accuracy: 0.7157 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3793 - accuracy: 0.9660 - val_loss: 2.0660 - val_accuracy: 0.6734 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.3698 - accuracy: 0.9693 - val_loss: 1.4584 - val_accuracy: 0.7609 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3675 - accuracy: 0.9699 - val_loss: 1.2568 - val_accuracy: 0.7615 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3748 - accuracy: 0.9673 - val_loss: 1.7292 - val_accuracy: 0.7029 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3818 - accuracy: 0.9643 - val_loss: 1.5072 - val_accuracy: 0.7405 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.3635 - accuracy: 0.9718 - val_loss: 1.9295 - val_accuracy: 0.6895 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3607 - accuracy: 0.9708 - val_loss: 1.4640 - val_accuracy: 0.7442 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3657 - accuracy: 0.9691 - val_loss: 1.5136 - val_accuracy: 0.7262 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3678 - accuracy: 0.9682 - val_loss: 1.8464 - val_accuracy: 0.7145 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.3645 - accuracy: 0.9697 - val_loss: 1.6362 - val_accuracy: 0.7103 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3053 - accuracy: 0.9914 - val_loss: 0.9879 - val_accuracy: 0.8281 - lr: 1.0000e-04\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2818 - accuracy: 0.9995 - val_loss: 1.0036 - val_accuracy: 0.8314 - lr: 1.0000e-04\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2745 - accuracy: 0.9999 - val_loss: 1.0359 - val_accuracy: 0.8309 - lr: 1.0000e-04\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.2685 - accuracy: 0.9999 - val_loss: 1.0687 - val_accuracy: 0.8300 - lr: 1.0000e-04\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.2627 - accuracy: 0.9999 - val_loss: 1.0966 - val_accuracy: 0.8309 - lr: 1.0000e-04\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.2560 - accuracy: 0.9999 - val_loss: 1.1312 - val_accuracy: 0.8312 - lr: 1.0000e-04\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2489 - accuracy: 0.9999 - val_loss: 1.1565 - val_accuracy: 0.8301 - lr: 1.0000e-04\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2408 - accuracy: 1.0000 - val_loss: 1.1643 - val_accuracy: 0.8308 - lr: 1.0000e-04\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2319 - accuracy: 1.0000 - val_loss: 1.1683 - val_accuracy: 0.8295 - lr: 1.0000e-04\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.2222 - accuracy: 1.0000 - val_loss: 1.1930 - val_accuracy: 0.8258 - lr: 1.0000e-04\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.2126 - accuracy: 0.9998 - val_loss: 1.2145 - val_accuracy: 0.8231 - lr: 1.0000e-04\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.2048 - accuracy: 0.9994 - val_loss: 1.3775 - val_accuracy: 0.8036 - lr: 1.0000e-04\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.1981 - accuracy: 0.9992 - val_loss: 1.2998 - val_accuracy: 0.8106 - lr: 1.0000e-04\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.1909 - accuracy: 0.9997 - val_loss: 1.2037 - val_accuracy: 0.8233 - lr: 1.0000e-04\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.1855 - accuracy: 0.9996 - val_loss: 1.2885 - val_accuracy: 0.8180 - lr: 1.0000e-04\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.1819 - accuracy: 0.9997 - val_loss: 1.2011 - val_accuracy: 0.8281 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.1806 - accuracy: 0.9999 - val_loss: 1.1974 - val_accuracy: 0.8291 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.1798 - accuracy: 1.0000 - val_loss: 1.1971 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.1789 - accuracy: 1.0000 - val_loss: 1.1939 - val_accuracy: 0.8307 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.1780 - accuracy: 0.9999 - val_loss: 1.1983 - val_accuracy: 0.8290 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.1769 - accuracy: 1.0000 - val_loss: 1.1946 - val_accuracy: 0.8293 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.1757 - accuracy: 1.0000 - val_loss: 1.1972 - val_accuracy: 0.8296 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.1744 - accuracy: 1.0000 - val_loss: 1.1960 - val_accuracy: 0.8298 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.1729 - accuracy: 1.0000 - val_loss: 1.1983 - val_accuracy: 0.8299 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.1712 - accuracy: 1.0000 - val_loss: 1.1968 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
            "Epoch 1/70\n",
            "390/390 [==============================] - 96s 230ms/step - loss: 1.9823 - accuracy: 0.4320 - val_loss: 2.8461 - val_accuracy: 0.2922 - lr: 0.0010\n",
            "Epoch 2/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 1.4471 - accuracy: 0.6163 - val_loss: 1.5887 - val_accuracy: 0.5879 - lr: 0.0010\n",
            "Epoch 3/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 1.2141 - accuracy: 0.6960 - val_loss: 1.3813 - val_accuracy: 0.6234 - lr: 0.0010\n",
            "Epoch 4/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 1.0796 - accuracy: 0.7375 - val_loss: 1.4554 - val_accuracy: 0.6080 - lr: 0.0010\n",
            "Epoch 5/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.9751 - accuracy: 0.7666 - val_loss: 1.2788 - val_accuracy: 0.6822 - lr: 0.0010\n",
            "Epoch 6/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.8998 - accuracy: 0.7877 - val_loss: 0.9913 - val_accuracy: 0.7607 - lr: 0.0010\n",
            "Epoch 7/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.8410 - accuracy: 0.8047 - val_loss: 1.1415 - val_accuracy: 0.7263 - lr: 0.0010\n",
            "Epoch 8/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.7895 - accuracy: 0.8180 - val_loss: 1.1122 - val_accuracy: 0.7221 - lr: 0.0010\n",
            "Epoch 9/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.7565 - accuracy: 0.8283 - val_loss: 1.3153 - val_accuracy: 0.6841 - lr: 0.0010\n",
            "Epoch 10/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.7236 - accuracy: 0.8370 - val_loss: 1.1647 - val_accuracy: 0.7130 - lr: 0.0010\n",
            "Epoch 11/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.6899 - accuracy: 0.8443 - val_loss: 0.8593 - val_accuracy: 0.7860 - lr: 0.0010\n",
            "Epoch 12/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.6639 - accuracy: 0.8531 - val_loss: 1.4023 - val_accuracy: 0.6617 - lr: 0.0010\n",
            "Epoch 13/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.6440 - accuracy: 0.8589 - val_loss: 1.5124 - val_accuracy: 0.6412 - lr: 0.0010\n",
            "Epoch 14/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.6212 - accuracy: 0.8656 - val_loss: 0.8130 - val_accuracy: 0.8130 - lr: 0.0010\n",
            "Epoch 15/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.6023 - accuracy: 0.8698 - val_loss: 0.8539 - val_accuracy: 0.7873 - lr: 0.0010\n",
            "Epoch 16/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.5898 - accuracy: 0.8728 - val_loss: 1.0520 - val_accuracy: 0.7584 - lr: 0.0010\n",
            "Epoch 17/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.5800 - accuracy: 0.8756 - val_loss: 1.0144 - val_accuracy: 0.7572 - lr: 0.0010\n",
            "Epoch 18/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.5645 - accuracy: 0.8809 - val_loss: 0.7845 - val_accuracy: 0.8100 - lr: 0.0010\n",
            "Epoch 19/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.5550 - accuracy: 0.8840 - val_loss: 0.8892 - val_accuracy: 0.7930 - lr: 0.0010\n",
            "Epoch 20/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5406 - accuracy: 0.8892 - val_loss: 0.9583 - val_accuracy: 0.7709 - lr: 0.0010\n",
            "Epoch 21/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.5321 - accuracy: 0.8908 - val_loss: 0.9568 - val_accuracy: 0.7763 - lr: 0.0010\n",
            "Epoch 22/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5231 - accuracy: 0.8940 - val_loss: 0.7375 - val_accuracy: 0.8264 - lr: 0.0010\n",
            "Epoch 23/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.5139 - accuracy: 0.8964 - val_loss: 0.7703 - val_accuracy: 0.8197 - lr: 0.0010\n",
            "Epoch 24/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5110 - accuracy: 0.8978 - val_loss: 0.6707 - val_accuracy: 0.8449 - lr: 0.0010\n",
            "Epoch 25/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.5002 - accuracy: 0.9010 - val_loss: 0.7887 - val_accuracy: 0.8092 - lr: 0.0010\n",
            "Epoch 26/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4954 - accuracy: 0.9011 - val_loss: 0.7952 - val_accuracy: 0.8068 - lr: 0.0010\n",
            "Epoch 27/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4894 - accuracy: 0.9036 - val_loss: 0.9503 - val_accuracy: 0.7791 - lr: 0.0010\n",
            "Epoch 28/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.4874 - accuracy: 0.9043 - val_loss: 0.7033 - val_accuracy: 0.8424 - lr: 0.0010\n",
            "Epoch 29/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4747 - accuracy: 0.9092 - val_loss: 1.2237 - val_accuracy: 0.7430 - lr: 0.0010\n",
            "Epoch 30/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4758 - accuracy: 0.9075 - val_loss: 0.6489 - val_accuracy: 0.8560 - lr: 0.0010\n",
            "Epoch 31/70\n",
            "390/390 [==============================] - 89s 226ms/step - loss: 0.4634 - accuracy: 0.9133 - val_loss: 0.7876 - val_accuracy: 0.8269 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4598 - accuracy: 0.9135 - val_loss: 1.0353 - val_accuracy: 0.7624 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.4531 - accuracy: 0.9160 - val_loss: 0.7472 - val_accuracy: 0.8369 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.4507 - accuracy: 0.9176 - val_loss: 0.6738 - val_accuracy: 0.8514 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.4522 - accuracy: 0.9166 - val_loss: 0.7438 - val_accuracy: 0.8301 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4420 - accuracy: 0.9198 - val_loss: 0.6942 - val_accuracy: 0.8465 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4387 - accuracy: 0.9198 - val_loss: 0.8970 - val_accuracy: 0.8097 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.4359 - accuracy: 0.9215 - val_loss: 0.8416 - val_accuracy: 0.8077 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.4393 - accuracy: 0.9206 - val_loss: 0.7727 - val_accuracy: 0.8368 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.4275 - accuracy: 0.9257 - val_loss: 0.8108 - val_accuracy: 0.8262 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4296 - accuracy: 0.9249 - val_loss: 0.7917 - val_accuracy: 0.8277 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4259 - accuracy: 0.9253 - val_loss: 0.8200 - val_accuracy: 0.8177 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4236 - accuracy: 0.9266 - val_loss: 0.7071 - val_accuracy: 0.8495 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 0.4189 - accuracy: 0.9284 - val_loss: 0.6627 - val_accuracy: 0.8589 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.4168 - accuracy: 0.9280 - val_loss: 0.8201 - val_accuracy: 0.8279 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.3380 - accuracy: 0.9570 - val_loss: 0.4862 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
            "Epoch 47/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.3033 - accuracy: 0.9684 - val_loss: 0.4762 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
            "Epoch 48/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2885 - accuracy: 0.9723 - val_loss: 0.4861 - val_accuracy: 0.9131 - lr: 1.0000e-04\n",
            "Epoch 49/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2792 - accuracy: 0.9745 - val_loss: 0.4984 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
            "Epoch 50/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2695 - accuracy: 0.9772 - val_loss: 0.5041 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
            "Epoch 51/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2623 - accuracy: 0.9782 - val_loss: 0.4995 - val_accuracy: 0.9173 - lr: 1.0000e-04\n",
            "Epoch 52/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.2565 - accuracy: 0.9802 - val_loss: 0.5068 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
            "Epoch 53/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2486 - accuracy: 0.9817 - val_loss: 0.5190 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
            "Epoch 54/70\n",
            "390/390 [==============================] - 89s 226ms/step - loss: 0.2436 - accuracy: 0.9822 - val_loss: 0.5230 - val_accuracy: 0.9177 - lr: 1.0000e-04\n",
            "Epoch 55/70\n",
            "390/390 [==============================] - 89s 226ms/step - loss: 0.2403 - accuracy: 0.9830 - val_loss: 0.5013 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
            "Epoch 56/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.2320 - accuracy: 0.9849 - val_loss: 0.5234 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
            "Epoch 57/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.2291 - accuracy: 0.9855 - val_loss: 0.5277 - val_accuracy: 0.9163 - lr: 1.0000e-04\n",
            "Epoch 58/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2264 - accuracy: 0.9852 - val_loss: 0.5182 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
            "Epoch 59/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.2215 - accuracy: 0.9868 - val_loss: 0.5453 - val_accuracy: 0.9127 - lr: 1.0000e-04\n",
            "Epoch 60/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.2168 - accuracy: 0.9877 - val_loss: 0.5402 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
            "Epoch 61/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2096 - accuracy: 0.9895 - val_loss: 0.5158 - val_accuracy: 0.9209 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2074 - accuracy: 0.9910 - val_loss: 0.5184 - val_accuracy: 0.9202 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2065 - accuracy: 0.9914 - val_loss: 0.5205 - val_accuracy: 0.9210 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2037 - accuracy: 0.9917 - val_loss: 0.5217 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.2040 - accuracy: 0.9915 - val_loss: 0.5253 - val_accuracy: 0.9219 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.2034 - accuracy: 0.9919 - val_loss: 0.5261 - val_accuracy: 0.9207 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2012 - accuracy: 0.9930 - val_loss: 0.5270 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 0.1997 - accuracy: 0.9927 - val_loss: 0.5321 - val_accuracy: 0.9211 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2001 - accuracy: 0.9925 - val_loss: 0.5337 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.1989 - accuracy: 0.9930 - val_loss: 0.5361 - val_accuracy: 0.9217 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "def learning_rate_schedule(epoch):\n",
        "    if epoch < 45:\n",
        "        return 1e-3\n",
        "    if epoch < 60:\n",
        "        return 1e-4\n",
        "    return 1e-5\n",
        "\n",
        "def train_with_lr_decay(model):\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy', metrics=['accuracy'],\n",
        "        optimizer=optimizers.Adam(lr=1e-3))\n",
        "\n",
        "    # Callback for learning rate adjustment\n",
        "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "    # TensorBoard callback\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
        "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "    \n",
        "    # Fit the model on the batches generated by datagen.flow()\n",
        "    model.fit(\n",
        "        x_train, y_train, batch_size=128,\n",
        "        validation_data=(x_test, y_test), epochs=70, verbose=1, \n",
        "        callbacks=[lr_scheduler, tensorboard_callback])\n",
        "    \n",
        "def train_with_lr_decay_and_augmentation(model):\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "        optimizer=optimizers.Adam(lr=1e-3))\n",
        "\n",
        "    # Callback for learning rate adjustment\n",
        "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "    # TensorBoard callback\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    logdir = os.path.join(log_root, f'{model.name}_augmented_{timestamp}')\n",
        "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "    # Data augmentation: flip and shift horizontally/vertically by max 4 pixels\n",
        "    datagen = kerasimage.ImageDataGenerator(\n",
        "        width_shift_range=4, height_shift_range=4,\n",
        "        horizontal_flip=True, fill_mode='constant')\n",
        "    \n",
        "    # y targets as one-hot vectors\n",
        "    y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    \n",
        "    # Fit the model on the batches generated by datagen.flow() using model.fit()\n",
        "    model.fit(\n",
        "        datagen.flow(x_train, y_train_onehot, batch_size=128),\n",
        "        validation_data=(x_test, y_test_onehot),\n",
        "        steps_per_epoch=len(x_train) / 128, epochs=70, verbose=1, \n",
        "        callbacks=[lr_scheduler, tensorboard_callback])\n",
        "\n",
        "resnet56 = resnet(56)\n",
        "train_with_lr_decay(resnet56)\n",
        "resnet56 = resnet(56)\n",
        "train_with_lr_decay_and_augmentation(resnet56)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Ib9dtROV0X"
      },
      "source": [
        "Q: Does the augmentation improve the final performance? What do you observe on the training and validation curves compared to no augmentation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn4oq1f3OV0X"
      },
      "source": [
        "Yes, the accuracy on the validation set gets better. The training accuracy grows slower due to the augmentation because the model can not overfit as easy anymore."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/tensorboard_logs_cnn02_01.zip /content/tensorboard_logs\n",
        "from google.colab import files\n",
        "files.download(\"/content/tensorboard_logs_cnn02_01.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Grh7oFJAGs6E",
        "outputId": "557a7764-9654-4033-f3f2-da997356f755"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/tensorboard_logs/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_augmented_20220403-183428/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_augmented_20220403-183428/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_augmented_20220403-183428/validation/events.out.tfevents.1649010956.0b7d6d0ca933.75.3.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/resnet56_augmented_20220403-183428/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_augmented_20220403-183428/train/events.out.tfevents.1649010868.0b7d6d0ca933.75.2.v2 (deflated 66%)\n",
            "  adding: content/tensorboard_logs/resnet56_20220403-165349/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_20220403-165349/validation/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_20220403-165349/validation/events.out.tfevents.1649004926.0b7d6d0ca933.75.1.v2 (deflated 78%)\n",
            "  adding: content/tensorboard_logs/resnet56_20220403-165349/train/ (stored 0%)\n",
            "  adding: content/tensorboard_logs/resnet56_20220403-165349/train/events.out.tfevents.1649004830.0b7d6d0ca933.75.0.v2 (deflated 66%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_41cbc8bb-d65c-4d47-a9f0-46ac7e85427b\", \"tensorboard_logs_cnn02_01.zip\", 9289245)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}